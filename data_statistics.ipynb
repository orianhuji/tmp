{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 0 examples [00:00, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting content from /home/orian/.cache/huggingface/datasets/downloads/c30d1c3fc72df68ffab064438c260799f73db11cdf4eeb1e48d431bc73ce07a6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 0 examples [00:01, ? examples/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 2\u001b[0m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwikipedia\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhe\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m20241201\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/Tokens2Words/.venv/lib/python3.10/site-packages/datasets/load.py:2154\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[1;32m   2153\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2154\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2162\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2163\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2164\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2165\u001b[0m )\n",
      "File \u001b[0;32m~/PycharmProjects/Tokens2Words/.venv/lib/python3.10/site-packages/datasets/builder.py:924\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    923\u001b[0m     prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m--> 924\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/PycharmProjects/Tokens2Words/.venv/lib/python3.10/site-packages/datasets/builder.py:1000\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    996\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m-> 1000\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   1003\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1004\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1005\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1006\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m   1007\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/Tokens2Words/.venv/lib/python3.10/site-packages/datasets/builder.py:1741\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1739\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1740\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[0;32m-> 1741\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m job_id, done, content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split_single(\n\u001b[1;32m   1742\u001b[0m         gen_kwargs\u001b[38;5;241m=\u001b[39mgen_kwargs, job_id\u001b[38;5;241m=\u001b[39mjob_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_prepare_split_args\n\u001b[1;32m   1743\u001b[0m     ):\n\u001b[1;32m   1744\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   1745\u001b[0m             result \u001b[38;5;241m=\u001b[39m content\n",
      "File \u001b[0;32m~/PycharmProjects/Tokens2Words/.venv/lib/python3.10/site-packages/datasets/builder.py:1854\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1853\u001b[0m     _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 1854\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, table \u001b[38;5;129;01min\u001b[39;00m generator:\n\u001b[1;32m   1855\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m max_shard_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m writer\u001b[38;5;241m.\u001b[39m_num_bytes \u001b[38;5;241m>\u001b[39m max_shard_size:\n\u001b[1;32m   1856\u001b[0m             num_examples, num_bytes \u001b[38;5;241m=\u001b[39m writer\u001b[38;5;241m.\u001b[39mfinalize()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/wikipedia/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia.py:1018\u001b[0m, in \u001b[0;36mWikipedia._generate_tables\u001b[0;34m(self, files, is_processed)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[1;32m   1017\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerating examples from = \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, file)\n\u001b[0;32m-> 1018\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m content \u001b[38;5;129;01min\u001b[39;00m _extract_content(file):\n\u001b[1;32m   1019\u001b[0m         example \u001b[38;5;241m=\u001b[39m _clean_content(content, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlanguage)\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m example \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/wikipedia/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia.py:1042\u001b[0m, in \u001b[0;36m_extract_content\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m   1040\u001b[0m utf_f \u001b[38;5;241m=\u001b[39m codecs\u001b[38;5;241m.\u001b[39mgetreader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)(f)\n\u001b[1;32m   1041\u001b[0m context \u001b[38;5;241m=\u001b[39m etree\u001b[38;5;241m.\u001b[39miterparse(utf_f, events\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m,))\n\u001b[0;32m-> 1042\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m unused_event, elem \u001b[38;5;129;01min\u001b[39;00m context:\n\u001b[1;32m   1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m elem\u001b[38;5;241m.\u001b[39mtag\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/xml/etree/ElementTree.py:1255\u001b[0m, in \u001b[0;36miterparse.<locals>.iterator\u001b[0;34m(source)\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m pullparser\u001b[38;5;241m.\u001b[39mread_events()\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;66;03m# load event buffer\u001b[39;00m\n\u001b[0;32m-> 1255\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m   1257\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/codecs.py:498\u001b[0m, in \u001b[0;36mStreamReader.read\u001b[0;34m(self, size, chars, firstline)\u001b[0m\n\u001b[1;32m    496\u001b[0m     newdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 498\u001b[0m     newdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# decode bytes (those remaining from the last call included)\u001b[39;00m\n\u001b[1;32m    500\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbytebuffer \u001b[38;5;241m+\u001b[39m newdata\n",
      "File \u001b[0;32m/usr/lib/python3.10/bz2.py:164\u001b[0m, in \u001b[0;36mBZ2File.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Read up to size uncompressed bytes from the file.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03mIf size is negative or omitted, read until EOF is reached.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03mReturns b'' if the file is already at EOF.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_can_read()\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[0;32m---> 68\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "File \u001b[0;32m/usr/lib/python3.10/_compression.py:103\u001b[0m, in \u001b[0;36mDecompressReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m         rawblock \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 103\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decompressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrawblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "load_dataset(\"wikipedia\", language=\"he\", date=\"20241201\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import xml.etree.ElementTree as etree\n",
    "\n",
    "\n",
    "pathWikiXML = '/home/orian/PycharmProjects/Tokens2Words/data/hewiki-latest-pages-articles.xml'\n",
    "\n",
    "\n",
    "def strip_tag_name(t):\n",
    "    return t.split(\"}\")[1] if \"}\" in t else t\n",
    "\n",
    "all_wiki_records = []\n",
    "\n",
    "cnt = 0\n",
    "for event, elem in etree.iterparse(pathWikiXML, events=('start', 'end')):\n",
    "    tname = strip_tag_name(elem.tag)\n",
    "\n",
    "    if event == 'start':\n",
    "        if tname == 'text':\n",
    "            text = elem.text\n",
    "            if text:\n",
    "                text = '\\n'.join([l for l in text.split('\\n') if not (l.startswith('<') or l.startswith('>') or l.startswith('[[') or l.startswith('{{') or l.startswith('__') or l.startswith('==') or l.startswith('*') or l.startswith('#') or l.startswith(\"'''\"))])\n",
    "                text = re.sub(r'\\|.+?\\]\\]', '', text).replace('[[', '').replace(']]', '').replace(\"'''\", '')\n",
    "                text = re.sub(r'\\<ref\\>.+?\\<\\/ref\\>', '', text).strip()\n",
    "\n",
    "                if text:\n",
    "                    all_wiki_records.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
    "model_name = 'Mistral'\n",
    "token = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_tokenizer_100 = tokenizer.train_new_from_iterator(iter(all_wiki_records[:100]), vocab_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "927"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(new_tokenizer_100.vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_tokenizer_30k = tokenizer.train_new_from_iterator(iter(all_wiki_records), vocab_size=30_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ğ’‚—',\n",
       " 'ğ’‚Ÿ',\n",
       " 'ğ’ƒ¶',\n",
       " 'ğ’„ˆ',\n",
       " 'ğ’„ ',\n",
       " 'ğ’„©',\n",
       " 'ğ’…',\n",
       " 'ğ’……',\n",
       " 'ğ’…',\n",
       " 'ğ’†',\n",
       " 'ğ’† ',\n",
       " 'ğ’†³',\n",
       " 'ğ’ˆ¥',\n",
       " 'ğ’ˆª',\n",
       " 'ğ’ˆ¬',\n",
       " 'ğ’ˆ¾',\n",
       " 'ğ’‰†',\n",
       " 'ğ’‰Œ',\n",
       " 'ğ’Š',\n",
       " 'ğ’Š©',\n",
       " 'ğ’‹—',\n",
       " 'ğ’‹¢',\n",
       " 'ğ’‹»',\n",
       " 'ğ’‹¾',\n",
       " 'ğ’Œ†',\n",
       " 'ğ’ŒŒ',\n",
       " 'ğ’',\n",
       " 'ğ“Ÿ',\n",
       " 'ğ“‚†',\n",
       " 'ğ“‚‹',\n",
       " 'ğ“‚»',\n",
       " 'ğ“„¤',\n",
       " 'ğ“…',\n",
       " 'ğ“†‘',\n",
       " 'ğ“‡‹',\n",
       " 'ğ“‡',\n",
       " 'ğ“ˆ‰',\n",
       " 'ğ“ˆ–',\n",
       " 'ğ“‰”',\n",
       " 'ğ“Š½',\n",
       " 'ğ“˜',\n",
       " 'ğ“',\n",
       " 'ğ“­',\n",
       " 'ğ„«',\n",
       " 'ğº',\n",
       " 'ğ‘œ',\n",
       " 'ğ’¶',\n",
       " 'ğ’¾',\n",
       " 'ğ“‚',\n",
       " 'ğ“‡',\n",
       " 'ğ“š',\n",
       " 'ğ“Ÿ',\n",
       " 'ğ“ª',\n",
       " 'ğ“­',\n",
       " 'ğ“®',\n",
       " 'ğ“²',\n",
       " 'ğ“µ',\n",
       " 'ğ“·',\n",
       " '\\U0001df0a',\n",
       " 'ğŸ‡¦',\n",
       " 'ğŸ‡§',\n",
       " 'ğŸ‡¨',\n",
       " 'ğŸ‡©',\n",
       " 'ğŸ‡ª',\n",
       " 'ğŸ‡«',\n",
       " 'ğŸ‡¬',\n",
       " 'ğŸ‡­',\n",
       " 'ğŸ‡®',\n",
       " 'ğŸ‡°',\n",
       " 'ğŸ‡±',\n",
       " 'ğŸ‡²',\n",
       " 'ğŸ‡³',\n",
       " 'ğŸ‡´',\n",
       " 'ğŸ‡µ',\n",
       " 'ğŸ‡·',\n",
       " 'ğŸ‡¸',\n",
       " 'ğŸ‡¹',\n",
       " 'ğŸ‡º',\n",
       " 'ğŸ‡»',\n",
       " 'ğŸ‡¾',\n",
       " 'ğŸ‡¿',\n",
       " 'ğŸŒˆ',\n",
       " 'ğŸŒ™',\n",
       " 'ğŸŒ',\n",
       " 'ğŸŒ«',\n",
       " 'ğŸŒ´',\n",
       " 'ğŸŒ·',\n",
       " 'ğŸŒ¸',\n",
       " 'ğŸŒ¹',\n",
       " 'ğŸŒ¼',\n",
       " 'ğŸ‚',\n",
       " 'ğŸ‰',\n",
       " 'ğŸ—',\n",
       " 'ğŸ¤',\n",
       " 'ğŸ§',\n",
       " 'ğŸ¬',\n",
       " 'ğŸ¶',\n",
       " 'ğŸ¸',\n",
       " 'ğŸ»',\n",
       " 'ğŸ†',\n",
       " 'ğŸ³',\n",
       " 'ğŸ¹',\n",
       " 'ğŸ»',\n",
       " 'ğŸ¼',\n",
       " 'ğŸ¦',\n",
       " 'ğŸ¾',\n",
       " 'ğŸ‘†',\n",
       " 'ğŸ‘‡',\n",
       " 'ğŸ‘‹',\n",
       " 'ğŸ‘',\n",
       " 'ğŸ‘',\n",
       " 'ğŸ‘»',\n",
       " 'ğŸ‘¼',\n",
       " 'ğŸ’ƒ',\n",
       " 'ğŸ’‹',\n",
       " 'ğŸ’',\n",
       " 'ğŸ’',\n",
       " 'ğŸ’‘',\n",
       " 'ğŸ’’',\n",
       " 'ğŸ’”',\n",
       " 'ğŸ’•',\n",
       " 'ğŸ’—',\n",
       " 'ğŸ’˜',\n",
       " 'ğŸ’™',\n",
       " 'ğŸ’š',\n",
       " 'ğŸ’›',\n",
       " 'ğŸ’œ',\n",
       " 'ğŸ’',\n",
       " 'ğŸ’¡',\n",
       " 'ğŸ’©',\n",
       " 'ğŸ’ª',\n",
       " 'ğŸ’«',\n",
       " 'ğŸ’­',\n",
       " 'ğŸ’¿',\n",
       " 'ğŸ“€',\n",
       " 'ğŸ“’',\n",
       " 'ğŸ“•',\n",
       " 'ğŸ“–',\n",
       " 'ğŸ“¸',\n",
       " 'ğŸ”„',\n",
       " 'ğŸ”‘',\n",
       " 'ğŸ”¥',\n",
       " 'ğŸ”ª',\n",
       " 'ğŸ•',\n",
       " 'ğŸ•µ',\n",
       " 'ğŸ•·',\n",
       " 'ğŸ–¤',\n",
       " 'ğŸ˜€',\n",
       " 'ğŸ˜',\n",
       " 'ğŸ˜‚',\n",
       " 'ğŸ˜Š',\n",
       " 'ğŸ˜',\n",
       " 'ğŸ˜‘',\n",
       " 'ğŸ˜œ',\n",
       " 'ğŸ˜³',\n",
       " 'ğŸ™‚',\n",
       " 'ğŸ™ˆ',\n",
       " 'ğŸ™',\n",
       " 'ğŸš†',\n",
       " 'ğŸš¨',\n",
       " 'ğŸ›¡',\n",
       " 'ğŸœ¨',\n",
       " 'ğŸ¤',\n",
       " 'ğŸ¤',\n",
       " 'ğŸ¤”',\n",
       " 'ğŸ¤˜',\n",
       " 'ğŸ¤¦',\n",
       " 'ğŸ¤ª',\n",
       " 'ğŸ¤·',\n",
       " 'ğŸ¥‚',\n",
       " 'ğŸ¥³',\n",
       " 'ğŸ¥º',\n",
       " 'ğŸ¦‹',\n",
       " 'ğŸ¦”',\n",
       " 'ğŸ§¡',\n",
       " 'ğŸ§¨',\n",
       " 'ğ †¯',\n",
       " 'ğ ˜¨',\n",
       " 'ğ ¥“',\n",
       " 'ğ£¯¶',\n",
       " 'ğ¤°“',\n",
       " 'ğ¤°”',\n",
       " 'ğ¥¥',\n",
       " 'â–×”',\n",
       " 'â–×‘',\n",
       " '×ªâ–',\n",
       " '×”â–',\n",
       " '×™×',\n",
       " 'â–×',\n",
       " '\\n|',\n",
       " 'â–×©',\n",
       " '×•×¨',\n",
       " 'â–×',\n",
       " 'â–×œ',\n",
       " '×•×ª',\n",
       " '× ×™',\n",
       " 'â–×•',\n",
       " '×¨×™',\n",
       " '×œ×™',\n",
       " '×¨×•',\n",
       " '}}',\n",
       " '×œâ–',\n",
       " '\\n\\n',\n",
       " '×™×•',\n",
       " '×ªâ–×”',\n",
       " '××•',\n",
       " 'â–×¢',\n",
       " '20',\n",
       " '{{',\n",
       " '× ×•',\n",
       " ',â–',\n",
       " '×œ×•',\n",
       " '×™×™',\n",
       " '×§×•',\n",
       " '××•',\n",
       " '×“×™',\n",
       " 'â–×›',\n",
       " '×‘×™',\n",
       " '×”â–×”',\n",
       " '×™×¨',\n",
       " '19',\n",
       " '×‘×¨',\n",
       " '×•×œ',\n",
       " '××™',\n",
       " '×”â–×‘',\n",
       " '×ª×•',\n",
       " '×‘×•',\n",
       " '×Ÿâ–',\n",
       " '×¤×¨',\n",
       " '××™',\n",
       " '×œâ–×”',\n",
       " '=â–',\n",
       " '××©',\n",
       " '.\\n\\n',\n",
       " '×“×•',\n",
       " '×¨×',\n",
       " '×¡×™',\n",
       " '11',\n",
       " '×¢×¨',\n",
       " '×¤×™',\n",
       " '×©×•',\n",
       " '×â–',\n",
       " '×—×•',\n",
       " '×§×™',\n",
       " '×™×â–',\n",
       " '×—×¨',\n",
       " '×˜×™',\n",
       " '×•×ªâ–×”',\n",
       " '×©×™',\n",
       " '×”â–×œ',\n",
       " '×•×',\n",
       " '×¡×•',\n",
       " '×¤×•',\n",
       " '×©× ',\n",
       " '××¨',\n",
       " 'â–×™',\n",
       " 'â–|',\n",
       " 'â–(',\n",
       " '×˜×•',\n",
       " '×ª×™',\n",
       " '×”â–×©',\n",
       " '×—×™',\n",
       " 'â–×‘×™',\n",
       " 'â–×‘×',\n",
       " 'â–×”×',\n",
       " '\\n|â–',\n",
       " ',â–×•',\n",
       " '×’×•',\n",
       " '×’×™',\n",
       " 'er',\n",
       " '×¦×™',\n",
       " 'â–× ',\n",
       " '×”×™',\n",
       " '×¢×•',\n",
       " '×’×¨',\n",
       " '×“×¨',\n",
       " '×¢×™',\n",
       " '×”×•',\n",
       " 'â–20',\n",
       " '×˜×¨',\n",
       " '×â–×”',\n",
       " '×Ÿâ–×”',\n",
       " '×ªâ–×©',\n",
       " '×§×¨',\n",
       " '×›×•',\n",
       " '.â–×‘',\n",
       " '× ×”â–',\n",
       " 'â–=â–',\n",
       " '00',\n",
       " '× ×”',\n",
       " 'â–×œ×',\n",
       " '.â–×”',\n",
       " '.â–',\n",
       " '×™×â–×‘',\n",
       " '×ªâ–×',\n",
       " '×ªâ–×',\n",
       " 'or',\n",
       " '×œ×',\n",
       " 'â–×©×œâ–',\n",
       " 'in',\n",
       " '×ª×¨',\n",
       " ',â–×”',\n",
       " '=\"',\n",
       " 'an',\n",
       " '\\n|-',\n",
       " '=\\n|',\n",
       " 'at',\n",
       " 'le',\n",
       " '×”×¢×¨',\n",
       " '×§×•×¨',\n",
       " '×–×™',\n",
       " '×”â–×',\n",
       " ',â–×',\n",
       " '×—×œ',\n",
       " '×©×¨',\n",
       " '× ×’',\n",
       " 'â–×©×œ',\n",
       " '×•×™',\n",
       " '×©× ×ªâ–',\n",
       " '×¦×¨',\n",
       " 'on',\n",
       " '{{×”×¢×¨',\n",
       " '× ×™×',\n",
       " '×¡×˜',\n",
       " '×¡×¤×¨',\n",
       " 'eâ–',\n",
       " 'ar',\n",
       " '×ª×',\n",
       " '×‘×¨×™',\n",
       " '×—×“',\n",
       " 'â–××•',\n",
       " 'al',\n",
       " '×ªâ–×œ',\n",
       " ',â–×‘',\n",
       " '××¨',\n",
       " 'en',\n",
       " 'â–×’',\n",
       " '×™×â–×•',\n",
       " '×›×™',\n",
       " 'st',\n",
       " '×â–×‘',\n",
       " 'â–×¤',\n",
       " '×—×§',\n",
       " '\\n}}',\n",
       " '×ªâ–×‘',\n",
       " '{{×©',\n",
       " 'â–×”×™',\n",
       " '×”|',\n",
       " '18',\n",
       " '{{×©}}',\n",
       " '×¡×¤',\n",
       " '×”â–×',\n",
       " ',â–×©',\n",
       " 'â–×©×œâ–×”',\n",
       " 'â–×œ×”',\n",
       " '×’×œ',\n",
       " 'â–×¢×œâ–',\n",
       " '.\\n\\n×‘',\n",
       " '×™×â–×”',\n",
       " '×¨×™×š',\n",
       " 'ht',\n",
       " '×‘×¢',\n",
       " 'â–××ªâ–×”',\n",
       " '××œ',\n",
       " '×¦×•',\n",
       " 'â–××•',\n",
       " 'â–×§',\n",
       " '\\n\\n|',\n",
       " '×•×¨×™',\n",
       " '×¡×¨',\n",
       " 'co',\n",
       " 'it',\n",
       " 'â–\"',\n",
       " '×•×ª×¨',\n",
       " '×•×ªâ–×‘',\n",
       " '×–×•',\n",
       " '×”â–×•',\n",
       " 're',\n",
       " 'â–××™',\n",
       " '×¦×',\n",
       " '× ×“',\n",
       " '×Ÿâ–×‘',\n",
       " '201',\n",
       " '×¨××©×•',\n",
       " '××•×¨',\n",
       " '×ª××¨×™×š',\n",
       " 'â–××©',\n",
       " 'ww',\n",
       " '×–×¨',\n",
       " '× ×˜',\n",
       " '|â–',\n",
       " 'â–×œ×',\n",
       " '×ªâ–×•',\n",
       " 'â–×”×•',\n",
       " '×©×',\n",
       " '×”â–×¢',\n",
       " '×“â–',\n",
       " '200',\n",
       " 'â–××ªâ–',\n",
       " '× ×¡',\n",
       " '×ª×‘',\n",
       " '{{×”×¢×¨×”|',\n",
       " '×ª×—',\n",
       " '×¨×‘',\n",
       " '×ª×¤',\n",
       " \"×’'\",\n",
       " '×›×œ',\n",
       " 'â–19',\n",
       " '×“×•×¨',\n",
       " ',â–×',\n",
       " '××§×•×¨',\n",
       " '×©×•×¨',\n",
       " '××',\n",
       " 'â–â–',\n",
       " '×©×¨×',\n",
       " '×¢×œ',\n",
       " '× ×™×ª',\n",
       " 'â–×©×™',\n",
       " 'â–×—',\n",
       " 'th',\n",
       " '×™×â–×',\n",
       " '×™×“×™',\n",
       " ':â–',\n",
       " '//',\n",
       " 'â–×¡',\n",
       " 'tp',\n",
       " '××œ',\n",
       " '×‘×•×¨',\n",
       " 'sâ–',\n",
       " 'http',\n",
       " '://',\n",
       " '×”=',\n",
       " '×©×œ',\n",
       " '×‘×',\n",
       " '×‘×œ',\n",
       " '-1',\n",
       " 'â–×–',\n",
       " '×§×˜',\n",
       " '×¤×•×¨',\n",
       " '×œ×“',\n",
       " '1|',\n",
       " 'â–×•×”',\n",
       " '×”.',\n",
       " '×¢×™×¨',\n",
       " '×Ÿâ–×',\n",
       " '×¤×¨×™',\n",
       " '× ×™×™',\n",
       " 'â–×œ×™',\n",
       " '×•×ª×•',\n",
       " 'â–×ª',\n",
       " '×ªâ–×¢',\n",
       " '×—×‘×¨',\n",
       " 'â–×“',\n",
       " 'ig',\n",
       " '×§×‘×•',\n",
       " '×“â–×”',\n",
       " '×ªâ–×›',\n",
       " '×¦×™×¨',\n",
       " '×â–×',\n",
       " '||',\n",
       " '×©× ×ªâ–19',\n",
       " '\"â–',\n",
       " '××',\n",
       " 'ro',\n",
       " 'â–{{×©}}',\n",
       " '×™×â–×©',\n",
       " '\\n!',\n",
       " '×¤× ×™',\n",
       " '={{',\n",
       " 'ic',\n",
       " ',â–×›',\n",
       " '×¢×•×œ',\n",
       " '××“',\n",
       " '×©×¤',\n",
       " '}}\\n|',\n",
       " 'id',\n",
       " 'la',\n",
       " '×šâ–×”',\n",
       " '10',\n",
       " 'â–×¨',\n",
       " '-19',\n",
       " 'â–××™',\n",
       " '× ×‘',\n",
       " '×©×‘',\n",
       " '×œ×—',\n",
       " '×™×â–×œ',\n",
       " '×Ÿâ–×œ',\n",
       " '×¡×¨×˜',\n",
       " '×›×¨',\n",
       " 'es',\n",
       " '× ×§',\n",
       " '1111',\n",
       " 'â–×”×•×',\n",
       " '×ª××•',\n",
       " 'am',\n",
       " '×Ÿâ–×©',\n",
       " \"×¦'\",\n",
       " \"''\",\n",
       " '×¤×œ',\n",
       " '×§×“',\n",
       " '12',\n",
       " 'â–×”×©',\n",
       " '×’×“',\n",
       " '}}\\n\\n',\n",
       " '×¦×¢',\n",
       " 'â–×œ××—×¨',\n",
       " '×œ×™×•',\n",
       " '××‘×¨',\n",
       " '×ª×¤×§×™',\n",
       " '×©×=',\n",
       " 'â–×‘×',\n",
       " 'â–×›×™',\n",
       " '×™×',\n",
       " 'fâ–',\n",
       " '×œâ–×',\n",
       " '×‘×“',\n",
       " '×œ×”',\n",
       " '</',\n",
       " 'ac',\n",
       " '×’×¨×¡',\n",
       " '22',\n",
       " '17',\n",
       " '×©×¨××œ',\n",
       " '×—×œ×§',\n",
       " 'il',\n",
       " '×¨×›',\n",
       " '×¡×§',\n",
       " '×Ÿâ–×•',\n",
       " '×ª×§',\n",
       " '×§×•×“',\n",
       " '×”â–(',\n",
       " 'el',\n",
       " '×”,â–',\n",
       " '15',\n",
       " '×œâ–×',\n",
       " '×¦×•×ªâ–×”',\n",
       " '×”â–×©×œâ–',\n",
       " '×©×™×¨',\n",
       " '× ×™×•',\n",
       " 'â–||â–',\n",
       " 'w.',\n",
       " '×™×•×•',\n",
       " '×•×ª=',\n",
       " 'â–××”',\n",
       " 'â–-',\n",
       " 'â–×™×¦×™×¨',\n",
       " '.\\n\\n×”',\n",
       " 'â–×”×',\n",
       " 'is',\n",
       " '×ª×•×‘',\n",
       " 'ent',\n",
       " '× ×•×¡',\n",
       " '16',\n",
       " '23',\n",
       " '×›×“×•×¨',\n",
       " '\\n|-\\n|',\n",
       " '×”×œ',\n",
       " 'â–××œ',\n",
       " 'â–×‘××•',\n",
       " 'www.',\n",
       " '13',\n",
       " 'â–×¢×œâ–×™×“×™',\n",
       " '×•×ª×™',\n",
       " 'â–×©×”',\n",
       " 'ur',\n",
       " '×ª××¨×™×šâ–×™×¦×™×¨',\n",
       " 'ch',\n",
       " ',â–×¢',\n",
       " '×ªâ–×™',\n",
       " '×¦×•×ªâ–×”×‘×¨×™',\n",
       " '.co',\n",
       " 'â–××¨',\n",
       " 'dâ–',\n",
       " '14',\n",
       " '11111|',\n",
       " '×–×•×¨',\n",
       " 'ion',\n",
       " '×¤×¢',\n",
       " '×›×Ÿ',\n",
       " '×¢×¦',\n",
       " '×§×‘×•×¦',\n",
       " '×™×™×ª',\n",
       " '×™×.',\n",
       " 'â–×œ×•',\n",
       " '×‘×¨×•',\n",
       " 'mat',\n",
       " '×¤×˜',\n",
       " '×“×¢',\n",
       " '×”â–×›',\n",
       " '×× ×™',\n",
       " 'â–×©× ',\n",
       " '\\n}}\\n\\n',\n",
       " 'un',\n",
       " '://www.',\n",
       " '21',\n",
       " 'et',\n",
       " '×× ',\n",
       " '×¤×¡',\n",
       " '\\nâ–',\n",
       " '24',\n",
       " '× ×‘×—×¨',\n",
       " '×‘×”',\n",
       " '×“×™×•',\n",
       " '××¤',\n",
       " '×§×¡',\n",
       " '×™×•×¦×¨',\n",
       " '×”)',\n",
       " '××¢',\n",
       " 'https',\n",
       " '×™×â–×',\n",
       " '×¡×•×’',\n",
       " '×”â–=â–',\n",
       " 'sp',\n",
       " 'yâ–',\n",
       " '.â–×”×•×',\n",
       " 'pg',\n",
       " '×ª=',\n",
       " '××Ÿ',\n",
       " 'â–×¢×•',\n",
       " '×× ',\n",
       " '××§×•×¨=',\n",
       " '×¦×•×¨',\n",
       " '×¦×‘',\n",
       " 'aâ–',\n",
       " '×ªâ–××—×¨',\n",
       " '.j',\n",
       " '× ×©',\n",
       " 'â–×‘×”',\n",
       " 'â–200',\n",
       " 'de',\n",
       " '\\n|â–×©',\n",
       " '×§×œ',\n",
       " '×â–×œ',\n",
       " '××”â–',\n",
       " '×§×•×¤',\n",
       " '×šâ–',\n",
       " '×¡×“×¨',\n",
       " 'â–×”×ª',\n",
       " '\"â–|',\n",
       " '×¢×“',\n",
       " '--',\n",
       " '××—',\n",
       " '× ×',\n",
       " '×¤×—',\n",
       " '.jpg',\n",
       " 'math',\n",
       " '×§×•×‘',\n",
       " 'â–=',\n",
       " '×©×ª',\n",
       " '×”\"',\n",
       " '×’×¨×¡××•',\n",
       " 'â–×‘×•',\n",
       " '×¨×™×™',\n",
       " '××“×™',\n",
       " '×˜×•×¨',\n",
       " '×’×•×¨',\n",
       " '×ª×¤×§×™×“',\n",
       " '×‘×•×“',\n",
       " '× ×•×ª',\n",
       " '×œ×™×™',\n",
       " '×–×™×§',\n",
       " '×™×â–×•×”×™',\n",
       " '××™×©×•×¨',\n",
       " '×ª×¨×™×',\n",
       " '×¨×•×ªâ–',\n",
       " '×”×¨',\n",
       " '×¢×œâ–',\n",
       " '××¡',\n",
       " '×—×“×©',\n",
       " 'ing',\n",
       " 'ol',\n",
       " '××•×¨',\n",
       " '×™×™×¨',\n",
       " '× ×”â–×‘',\n",
       " 'yle',\n",
       " 'â–× ×•',\n",
       " '×“×’×œ',\n",
       " '×˜×œ',\n",
       " '×ª×™××•×¨',\n",
       " '××™×©×•×¨×™×â–×•×”×™',\n",
       " '××™×©×•×¨×™×â–×•×”×™×ª×¨×™×',\n",
       " 'math>',\n",
       " '×¤×¨×•',\n",
       " '×’×¨×¡××•×ªâ–××—×¨',\n",
       " '× ×ªâ–',\n",
       " 'â–×›×œ',\n",
       " '.com',\n",
       " 'â–â€¢',\n",
       " 'style',\n",
       " '×’×¨×¡××•×ªâ–××—×¨×•×ª=',\n",
       " '×©×—×§',\n",
       " '×¨×•×‘',\n",
       " '|\\n|',\n",
       " '×ªâ–×©×œâ–',\n",
       " 'ign',\n",
       " '× ×¨',\n",
       " '× ×•×ªâ–',\n",
       " '×—×¨×™',\n",
       " '30',\n",
       " '×¤×¨×¡',\n",
       " 'to',\n",
       " '×â–×',\n",
       " '×—×‘',\n",
       " 'style=\"',\n",
       " ',â–×œ',\n",
       " '×›×•×œ',\n",
       " '×—×•×–',\n",
       " '×¨×™×§',\n",
       " 'â–×¢×â–',\n",
       " 'ik',\n",
       " '×©×¤×ªâ–×”',\n",
       " '×¨××©',\n",
       " '×”â–×™',\n",
       " '\\t\\t',\n",
       " '×’×¨×¡××•×ªâ–××—×¨×•×ª=\\n}}',\n",
       " '|×›',\n",
       " 'â–×©×•',\n",
       " 'â–×‘×¨',\n",
       " '××ªâ–×”',\n",
       " 'â–××',\n",
       " 'col',\n",
       " 'â–{{',\n",
       " '}}}}',\n",
       " 'â–××•×¨',\n",
       " '×œ×™×˜',\n",
       " '×¡×™×˜',\n",
       " 'â–×¦',\n",
       " 'ab',\n",
       " 'â–×•×‘',\n",
       " '44',\n",
       " 'â–×•×™',\n",
       " '×ª×•×›',\n",
       " '×™×•×ªâ–',\n",
       " '×©×•×‘',\n",
       " '× ×™×‘×¨',\n",
       " '{{×”×¢×¨×”|{{',\n",
       " '×Ÿâ–(',\n",
       " '×–×›',\n",
       " 'â–×™×•',\n",
       " '199',\n",
       " '25',\n",
       " '×â–×‘×©×¤×ªâ–×”',\n",
       " '×§×•×œ',\n",
       " 'ss',\n",
       " '× ×”â–×”',\n",
       " '× ×—',\n",
       " '×¢×¨×›',\n",
       " 'align',\n",
       " 'ir',\n",
       " '××”',\n",
       " '×œ×™×’',\n",
       " '100',\n",
       " 'â–×”×¢',\n",
       " '× ×•×¢',\n",
       " 'iv',\n",
       " 'ed',\n",
       " 'â–×¢×œâ–×”',\n",
       " '××œ×™',\n",
       " '××•×©',\n",
       " '× ×™×‘×¨×¡×™×˜',\n",
       " ',â–×•×”',\n",
       " '×§×¨×™',\n",
       " 'â–×‘×™×•',\n",
       " '××™×',\n",
       " 'Th',\n",
       " '×©×',\n",
       " '×¡×œ',\n",
       " 'â–×‘×ª',\n",
       " '×”â–×œ×”',\n",
       " '×”â–×©×œâ–×”',\n",
       " '×Ÿâ–×',\n",
       " '×‘×™×‘',\n",
       " '×’×™×¢',\n",
       " 'â–S',\n",
       " '7%',\n",
       " '×§×‘',\n",
       " '×¡×˜×™',\n",
       " '××ª×¨',\n",
       " '×œ××•',\n",
       " 'us',\n",
       " 'wid',\n",
       " '>{{',\n",
       " '× ×¤',\n",
       " 'width',\n",
       " 'ad',\n",
       " '××™×©×•×¨×™×â–×•×”×™×ª×¨×™×={{',\n",
       " 'ag',\n",
       " '×›×‘',\n",
       " '××§×•',\n",
       " '××©×¨',\n",
       " '×¢×‘×¨',\n",
       " '-s',\n",
       " '\\n|â–×',\n",
       " '×œâ–×‘',\n",
       " '.com/',\n",
       " '× ×•×™',\n",
       " '50',\n",
       " '\\n|×ª××¨×™×šâ–×™×¦×™×¨',\n",
       " '×¡×™×™',\n",
       " '×“×¨×•',\n",
       " 'â–×”×¦',\n",
       " 'â–××›',\n",
       " 'ofâ–',\n",
       " '}}\\n|×’×¨×¡××•×ªâ–××—×¨×•×ª=\\n}}',\n",
       " ',â–×•×‘',\n",
       " '{|â–',\n",
       " '×¦×¨×¤',\n",
       " '×›×“×•×¨×’×œ',\n",
       " '×ª×‘× ×™×ª',\n",
       " '\\n|-\\n|â–',\n",
       " 'â–â–â–â–',\n",
       " 'â–×¢×“',\n",
       " '11111|11111|',\n",
       " '×—×™×“',\n",
       " '×”â–×‘×™',\n",
       " '×—×™×œ',\n",
       " 'â–×˜',\n",
       " '×ª.',\n",
       " '×œ×•×©',\n",
       " 'â–|×ª×™××•×¨',\n",
       " '×‘×™×ªâ–×”',\n",
       " 'â–×‘×©',\n",
       " '× ×˜×™',\n",
       " '×–×',\n",
       " '×›× ×¡',\n",
       " 'â–×—×•',\n",
       " '×ª×‘× ×™×ª:',\n",
       " '197',\n",
       " '×’×“×•×œ',\n",
       " 'lo',\n",
       " '×Ÿ.',\n",
       " '×šâ–×œ',\n",
       " '× ×™×”',\n",
       " '×â–×©',\n",
       " '××“',\n",
       " '×Ÿâ–×›',\n",
       " 'ew',\n",
       " 'â–×¤×¨',\n",
       " '×œ×‘',\n",
       " 'eb',\n",
       " '××•×“',\n",
       " '×œ×•×’×™',\n",
       " 'em',\n",
       " 'â–×”×¨',\n",
       " '××ª',\n",
       " '×’×¨×× ×™',\n",
       " '90',\n",
       " '×œ××—×¨',\n",
       " 'D7%',\n",
       " '×Ÿ,â–',\n",
       " '××©×—×§',\n",
       " 'ra',\n",
       " 'no',\n",
       " 'â–×”×¤',\n",
       " '×”â–×©×œ',\n",
       " '×“×’×œ|',\n",
       " '000',\n",
       " '×˜×™×¤',\n",
       " 'â–×‘×¢',\n",
       " '26',\n",
       " 'â–×¨×•',\n",
       " '×§×•×‘×¥',\n",
       " '×”×•×“',\n",
       " '×”×‘',\n",
       " '×™×â–×›',\n",
       " '××ªâ–',\n",
       " ').',\n",
       " '××™×¨×•',\n",
       " '× ×”â–×œ',\n",
       " 'â–×¡×™',\n",
       " 'â–M',\n",
       " 'â–×”×¨××©×•',\n",
       " 'â–×•×',\n",
       " '×â–=â–',\n",
       " 'â–××˜×¨',\n",
       " '40',\n",
       " '×¤×•×œ×™',\n",
       " '×—×¡',\n",
       " '×§×¨×‘',\n",
       " '×”â–×”×',\n",
       " '80',\n",
       " 'â–|×ª×™××•×¨=',\n",
       " 'â–×§×™',\n",
       " '%D7%',\n",
       " 'im',\n",
       " '×“×•×¢',\n",
       " '<re',\n",
       " '× ×’×œ×™',\n",
       " '×•×¨×•',\n",
       " 'ame',\n",
       " '×¦×•×ªâ–×”×‘×¨×™×ª',\n",
       " 'â–×§×•',\n",
       " '×â–×›',\n",
       " 'â–A',\n",
       " '× ×™×¡',\n",
       " '× ×•×¡×£',\n",
       " 'der',\n",
       " '×”=\\n|',\n",
       " '-20',\n",
       " '×¡×',\n",
       " '××•×¢',\n",
       " 'â–×¢×œ',\n",
       " '×ª××©',\n",
       " 'â–×©×‘',\n",
       " '×©×˜',\n",
       " 'â–×”×™×',\n",
       " '27',\n",
       " '×—×¨×•',\n",
       " 'wik',\n",
       " '××‘×¨â–20',\n",
       " '×¤×¢×™',\n",
       " '×”×•×“×™',\n",
       " 'â–C',\n",
       " '× ×ª',\n",
       " '×•×ª.',\n",
       " '\\n|â–×”',\n",
       " '{{×”×¢×¨×”|×©×=',\n",
       " '×ª×§×•×¤',\n",
       " '× ×™×ªâ–',\n",
       " '××œ×‘×•',\n",
       " '× ×•×‘',\n",
       " \"×–'\",\n",
       " '×¤×',\n",
       " '× ×¡×™',\n",
       " '×–×',\n",
       " '1.',\n",
       " '×œ××•××™',\n",
       " '×›×œ×•',\n",
       " '28',\n",
       " 'om',\n",
       " '×ªâ–×©×œâ–×”',\n",
       " '-0',\n",
       " '× ×˜×¨',\n",
       " '×¢×©',\n",
       " '×§×•×‘×¥:',\n",
       " '\\n|}',\n",
       " '×™×,â–',\n",
       " '×¡×˜×¨',\n",
       " 'all',\n",
       " '×¤×™×¢',\n",
       " '×”â–×¢×œâ–',\n",
       " 'lu',\n",
       " 'â–××ª',\n",
       " 'â–3',\n",
       " '×™×•×¦×¨=',\n",
       " 'â–×‘-',\n",
       " 'http://www.',\n",
       " '×—×™×™',\n",
       " '××¨×¥',\n",
       " ')â–{{×©}}',\n",
       " '×©×™×',\n",
       " '×¡×•×£',\n",
       " 'â–×”×§',\n",
       " '×˜×¨×•',\n",
       " 'tr',\n",
       " '×¨×§',\n",
       " '×›×¡',\n",
       " \"â–×’'\",\n",
       " 'ate',\n",
       " '29',\n",
       " '×ªâ–(',\n",
       " '×›×ª×‘',\n",
       " 'as',\n",
       " '\"×',\n",
       " '60',\n",
       " '×¨×©',\n",
       " '×¨×™×•',\n",
       " '×•×™×–×™',\n",
       " '× ×•×¡×¤',\n",
       " '×™×™×¡',\n",
       " '××¦×',\n",
       " '×”×ª',\n",
       " '×”â–× ',\n",
       " '×§×™×©×•×¨',\n",
       " 'â–|â–×©',\n",
       " 'â–×‘××”×œ',\n",
       " '×’×•×¡×˜',\n",
       " 'ter',\n",
       " '×©× ×™',\n",
       " '×™×â–(',\n",
       " 'width=\"',\n",
       " 'â–×‘×™×•×ª×¨',\n",
       " '×˜×•×‘',\n",
       " 'name',\n",
       " '×¨×‘×™',\n",
       " '198',\n",
       " 'Theâ–',\n",
       " 'â–×©× ×™',\n",
       " '×•×¨×˜',\n",
       " '×¦×‘×¢',\n",
       " 'â–× ×™',\n",
       " '××‘',\n",
       " '×’×•×‘',\n",
       " '×ªâ–×”×',\n",
       " '× ×•×ªâ–×”',\n",
       " '××©×š',\n",
       " '×§×¨×•',\n",
       " 's/',\n",
       " 'enter',\n",
       " '×“×‘×¨',\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_vocab = {v: k for k,v in new_tokenizer_30k.vocab.items()}\n",
    "list(dict(sorted(rev_vocab.items(), key=lambda item: item[0], reverse=False)).values())[11_000:13_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11183"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer_30k.vocab['â–×”']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "my_counter = Counter()\n",
    "\n",
    "vocab = dict(new_tokenizer_30k.vocab)\n",
    "\n",
    "for record in all_wiki_records:\n",
    "    tokenized = new_tokenizer_30k.tokenize(record)\n",
    "    my_counter.update([t for t in tokenized if vocab[t] > 11182])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tkn</th>\n",
       "      <th>cnt</th>\n",
       "      <th>tkn_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4425</th>\n",
       "      <td>&lt;/</td>\n",
       "      <td>3613</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4430</th>\n",
       "      <td>--</td>\n",
       "      <td>3609</td>\n",
       "      <td>458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5380</th>\n",
       "      <td>×’×¨×¡××•</td>\n",
       "      <td>2983</td>\n",
       "      <td>468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6131</th>\n",
       "      <td>×™×â–×•×”×™</td>\n",
       "      <td>2619</td>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7069</th>\n",
       "      <td>w.</td>\n",
       "      <td>2252</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7076</th>\n",
       "      <td>×§×‘×•</td>\n",
       "      <td>2250</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7089</th>\n",
       "      <td>××™×©×•×¨</td>\n",
       "      <td>2246</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7382</th>\n",
       "      <td>×¦×•×ªâ–×”×‘×¨×™</td>\n",
       "      <td>2159</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7613</th>\n",
       "      <td>×ª×¨×™×</td>\n",
       "      <td>2091</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8463</th>\n",
       "      <td>//</td>\n",
       "      <td>1866</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8698</th>\n",
       "      <td>www.</td>\n",
       "      <td>1812</td>\n",
       "      <td>377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9744</th>\n",
       "      <td>math</td>\n",
       "      <td>1607</td>\n",
       "      <td>463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10108</th>\n",
       "      <td>1111</td>\n",
       "      <td>1542</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10301</th>\n",
       "      <td>.j</td>\n",
       "      <td>1512</td>\n",
       "      <td>443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10366</th>\n",
       "      <td>××§×•×¨=</td>\n",
       "      <td>1500</td>\n",
       "      <td>438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11126</th>\n",
       "      <td>ww</td>\n",
       "      <td>1391</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11366</th>\n",
       "      <td>×ª××¨×™×šâ–×™×¦×™×¨</td>\n",
       "      <td>1360</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11543</th>\n",
       "      <td>{{×©</td>\n",
       "      <td>1336</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12729</th>\n",
       "      <td>.co</td>\n",
       "      <td>1202</td>\n",
       "      <td>388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12898</th>\n",
       "      <td>pg</td>\n",
       "      <td>1186</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14438</th>\n",
       "      <td>tp</td>\n",
       "      <td>1054</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16135</th>\n",
       "      <td>yle</td>\n",
       "      <td>937</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17614</th>\n",
       "      <td>××™×©×•×¨×™×â–×•×”×™×ª×¨×™×</td>\n",
       "      <td>699</td>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17938</th>\n",
       "      <td>×ª×¤×§×™</td>\n",
       "      <td>416</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17956</th>\n",
       "      <td>11111|</td>\n",
       "      <td>403</td>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18316</th>\n",
       "      <td>http</td>\n",
       "      <td>148</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18436</th>\n",
       "      <td>https</td>\n",
       "      <td>83</td>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18466</th>\n",
       "      <td>://</td>\n",
       "      <td>66</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18494</th>\n",
       "      <td>××™×©×•×¨×™×â–×•×”×™</td>\n",
       "      <td>55</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18619</th>\n",
       "      <td>://www.</td>\n",
       "      <td>14</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   tkn   cnt  tkn_id\n",
       "4425                </  3613     335\n",
       "4430                --  3609     458\n",
       "5380             ×’×¨×¡××•  2983     468\n",
       "6131            ×™×â–×•×”×™  2619     479\n",
       "7069                w.  2252     358\n",
       "7076               ×§×‘×•  2250     272\n",
       "7089             ××™×©×•×¨  2246     480\n",
       "7382          ×¦×•×ªâ–×”×‘×¨×™  2159     387\n",
       "7613              ×ª×¨×™×  2091     481\n",
       "8463                //  1866     241\n",
       "8698              www.  1812     377\n",
       "9744              math  1607     463\n",
       "10108             1111  1542     309\n",
       "10301               .j  1512     443\n",
       "10366            ××§×•×¨=  1500     438\n",
       "11126               ww  1391     205\n",
       "11366       ×ª××¨×™×šâ–×™×¦×™×¨  1360     383\n",
       "11543              {{×©  1336     163\n",
       "12729              .co  1202     388\n",
       "12898               pg  1186     433\n",
       "14438               tp  1054     243\n",
       "16135              yle   937     492\n",
       "17614  ××™×©×•×¨×™×â–×•×”×™×ª×¨×™×   699     498\n",
       "17938             ×ª×¤×§×™   416     326\n",
       "17956           11111|   403     392\n",
       "18316             http   148     247\n",
       "18436            https    83     426\n",
       "18466              ://    66     248\n",
       "18494      ××™×©×•×¨×™×â–×•×”×™    55     497\n",
       "18619          ://www.    14     411"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "as_df = pd.DataFrame(my_counter.most_common(30_000), columns=['tkn','cnt'])\n",
    "# as_df['cnt'].hist()\n",
    "\n",
    "as_df['tkn_id'] = as_df['tkn'].apply(lambda t: vocab[t] - 11182)\n",
    "\n",
    "# as_df[(as_df['cnt'] < 1_000) & (as_df['cnt'] > 800)]['tkn']\n",
    "\n",
    "\n",
    "\n",
    "as_df[(as_df['cnt'] < 4000) & (as_df['tkn_id'] < 500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocab_pos</th>\n",
       "      <th>less_frequent_rank</th>\n",
       "      <th>tkn</th>\n",
       "      <th>cnt</th>\n",
       "      <th>unique_but_start_dict_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>248</td>\n",
       "      <td>254</td>\n",
       "      <td>://</td>\n",
       "      <td>66</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>247</td>\n",
       "      <td>404</td>\n",
       "      <td>http</td>\n",
       "      <td>148</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>411</td>\n",
       "      <td>101</td>\n",
       "      <td>://www.</td>\n",
       "      <td>14</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>426</td>\n",
       "      <td>284</td>\n",
       "      <td>https</td>\n",
       "      <td>83</td>\n",
       "      <td>509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>497</td>\n",
       "      <td>226</td>\n",
       "      <td>××™×©×•×¨×™×â–×•×”×™</td>\n",
       "      <td>55</td>\n",
       "      <td>552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18716</th>\n",
       "      <td>42</td>\n",
       "      <td>18716</td>\n",
       "      <td>×”â–×‘</td>\n",
       "      <td>238732</td>\n",
       "      <td>238774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18717</th>\n",
       "      <td>28</td>\n",
       "      <td>18717</td>\n",
       "      <td>,â–</td>\n",
       "      <td>245319</td>\n",
       "      <td>245347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18718</th>\n",
       "      <td>11</td>\n",
       "      <td>18718</td>\n",
       "      <td>â–×œ</td>\n",
       "      <td>270323</td>\n",
       "      <td>270334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18719</th>\n",
       "      <td>1</td>\n",
       "      <td>18719</td>\n",
       "      <td>â–×”</td>\n",
       "      <td>285420</td>\n",
       "      <td>285421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18720</th>\n",
       "      <td>2</td>\n",
       "      <td>18720</td>\n",
       "      <td>â–×‘</td>\n",
       "      <td>321352</td>\n",
       "      <td>321354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18721 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       vocab_pos  less_frequent_rank          tkn     cnt  \\\n",
       "254          248                 254          ://      66   \n",
       "404          247                 404         http     148   \n",
       "101          411                 101      ://www.      14   \n",
       "284          426                 284        https      83   \n",
       "226          497                 226  ××™×©×•×¨×™×â–×•×”×™      55   \n",
       "...          ...                 ...          ...     ...   \n",
       "18716         42               18716          ×”â–×‘  238732   \n",
       "18717         28               18717           ,â–  245319   \n",
       "18718         11               18718           â–×œ  270323   \n",
       "18719          1               18719           â–×”  285420   \n",
       "18720          2               18720           â–×‘  321352   \n",
       "\n",
       "       unique_but_start_dict_rank  \n",
       "254                           314  \n",
       "404                           395  \n",
       "101                           425  \n",
       "284                           509  \n",
       "226                           552  \n",
       "...                           ...  \n",
       "18716                      238774  \n",
       "18717                      245347  \n",
       "18718                      270334  \n",
       "18719                      285421  \n",
       "18720                      321354  \n",
       "\n",
       "[18721 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_idx = [(idx, item[0], item[1]) for idx,item in enumerate(reversed(my_counter.most_common(30_000)))]\n",
    "\n",
    "with_vocab_pos = [(vocab[item[1]] - 11182, item[0], item[1], item[2]) for item in with_idx]\n",
    "\n",
    "cnt_df = pd.DataFrame(with_vocab_pos, columns=['vocab_pos', 'less_frequent_rank', 'tkn', 'cnt'])\n",
    "cnt_df['unique_but_start_dict_rank'] = cnt_df.apply(lambda x: x['vocab_pos'] + x['cnt'], axis=1)\n",
    "\n",
    "\n",
    "cnt_df.sort_values('unique_but_start_dict_rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/30k_tokenizer/tokenizer_config.json',\n",
       " 'data/30k_tokenizer/special_tokens_map.json',\n",
       " 'data/30k_tokenizer/tokenizer.model',\n",
       " 'data/30k_tokenizer/added_tokens.json',\n",
       " 'data/30k_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(cnt_df, open('data/cnt_df.pkl', 'wb'))\n",
    "new_tokenizer_30k.save_pretrained('data/30k_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "cnt_df = pickle.load(open('data/cnt_df.pkl', 'rb'))\n",
    "new_tokenizer_30k = AutoTokenizer.from_pretrained('data/30k_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m      8\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# model_path = 'meta-llama/Llama-3.1-8B'\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# model_name = 'llama_3_1'\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[43mmodel_path\u001b[49m, token\u001b[38;5;241m=\u001b[39mtoken, torch_dtype\u001b[38;5;241m=\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path, token\u001b[38;5;241m=\u001b[39mtoken)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_path' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from src.tokens2words.word_retriever import AnalysisWordRetriever\n",
    "from src.tokens2words.utils.enums import MultiTokenKind\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# model_path = 'meta-llama/Llama-3.1-8B'\n",
    "# model_name = 'llama_3_1'\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, token=token, torch_dtype=dtype).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_retriever = AnalysisWordRetriever(model, tokenizer, MultiTokenKind.Natural, add_context=True,\n",
    "                                model_name=model_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_cnt = cnt_df[~cnt_df['tkn'].str.contains('â–')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_token_layers(record):\n",
    "    original_tkn = record['tkn']\n",
    "    tokenized_combined_text = tokenizer([original_tkn], return_tensors='pt', truncation=True, max_length=5).to(device)\n",
    "\n",
    "    predicted_layers = []\n",
    "    predicted_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized_combined_text, output_hidden_states=True)\n",
    "\n",
    "    hidden_states = outputs.hidden_states\n",
    "    for layer_idx, hidden_state in enumerate(hidden_states):\n",
    "        postfix_hidden_state = hidden_states[layer_idx][0, -1, :].unsqueeze(0)\n",
    "        retrieved_word_str = word_retriever.retriever.retrieve_word(postfix_hidden_state, layer_idx=layer_idx,\n",
    "                                                            num_tokens_to_generate=tokenized_combined_text['input_ids'].shape[1])\n",
    "        if retrieved_word_str[0].strip() == original_tkn:\n",
    "            predicted_layers.append(layer_idx)\n",
    "        predicted_values.append(original_tkn)\n",
    "    record['predicted_layers'] = predicted_layers\n",
    "    record['predicted_values'] = predicted_values\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_cnt = filtered_cnt.apply(get_token_layers, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(filtered_cnt, open('data/filtered_cnt.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "from collections import Counter\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from src.tokens2words.word_retriever import AnalysisWordRetriever\n",
    "from src.tokens2words.utils.enums import MultiTokenKind\n",
    "\n",
    "model_path = 'meta-llama/Llama-3.1-8B'\n",
    "model_name = 'llama_3_1'\n",
    "token = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, token=token)\n",
    "new_tokenizer_30k = tokenizer.train_new_from_iterator(iter(all_wiki_records), vocab_size=30_000)\n",
    "rev_vocab = {v: k for k,v in new_tokenizer_30k.vocab.items()}\n",
    "\n",
    "\n",
    "my_counter = Counter()\n",
    "\n",
    "vocab = dict(new_tokenizer_30k.vocab)\n",
    "\n",
    "# original_vocab_token_cnt = 11182\n",
    "\n",
    "original_vocab_token_cnt = 0\n",
    "for tkn in list(dict(sorted(rev_vocab.items(), key=lambda item: item[0], reverse=False)).values()):\n",
    "    original_vocab_token_cnt += 1\n",
    "    if len([t for t in tokenizer.encode(tkn) if t != 128000]) > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/llama3_30k_tokenizer/tokenizer_config.json',\n",
       " 'data/llama3_30k_tokenizer/special_tokens_map.json',\n",
       " 'data/llama3_30k_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for record in all_wiki_records:\n",
    "    tokenized = new_tokenizer_30k.tokenize(record)\n",
    "    my_counter.update([t for t in tokenized if vocab[t] > original_vocab_token_cnt])\n",
    "\n",
    "with_idx = [(idx, item[0], item[1]) for idx,item in enumerate(reversed(my_counter.most_common(30_000)))]\n",
    "\n",
    "with_vocab_pos = [(vocab[item[1]] - original_vocab_token_cnt, item[0], item[1], item[2]) for item in with_idx]\n",
    "\n",
    "cnt_df = pd.DataFrame(with_vocab_pos, columns=['vocab_pos', 'less_frequent_rank', 'tkn', 'cnt'])\n",
    "cnt_df['unique_but_start_dict_rank'] = cnt_df.apply(lambda x: x['vocab_pos'] + x['cnt'], axis=1)\n",
    "\n",
    "\n",
    "cnt_df.sort_values('unique_but_start_dict_rank')\n",
    "\n",
    "\n",
    "pickle.dump(cnt_df, open('data/llama3_cnt_df.pkl', 'wb'))\n",
    "new_tokenizer_30k.save_pretrained('data/llama3_30k_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_df = pickle.load(open('data/llama3_cnt_df.pkl', 'rb'))\n",
    "new_tokenizer_30k = AutoTokenizer.from_pretrained('data/llama3_30k_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnt_df['original_tkn'] = cnt_df['vocab_pos'].apply(lambda vp: rev_vocab[vp + original_vocab_token_cnt])\n",
    "cnt_df['original_tkn'] = cnt_df['tkn']\n",
    "\n",
    "vocab = new_tokenizer_30k.vocab\n",
    "\n",
    "cnt_df['tkn'] = cnt_df['original_tkn'].apply(lambda t: new_tokenizer_30k.decode(vocab[t]))\n",
    "\n",
    "filtered_cnt = cnt_df[~cnt_df['tkn'].str.contains('ï¿½')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc3ba3945904f358a74b1629651a10a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, token=token, torch_dtype=dtype).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, token=token)\n",
    "\n",
    "word_retriever = AnalysisWordRetriever(model, tokenizer, MultiTokenKind.Natural, add_context=True,\n",
    "                                model_name=model_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_layers(record):\n",
    "    original_tkn = record['tkn']\n",
    "    tokenized_combined_text = tokenizer([original_tkn], return_tensors='pt', truncation=True, max_length=5).to(device)\n",
    "\n",
    "    predicted_layers = []\n",
    "    predicted_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized_combined_text, output_hidden_states=True)\n",
    "\n",
    "    hidden_states = outputs.hidden_states\n",
    "    for layer_idx, hidden_state in enumerate(hidden_states):\n",
    "        postfix_hidden_state = hidden_states[layer_idx][0, -1, :].unsqueeze(0)\n",
    "        retrieved_word_str = word_retriever.retriever.retrieve_word(postfix_hidden_state, layer_idx=layer_idx,\n",
    "                                                            num_tokens_to_generate=tokenized_combined_text['input_ids'].shape[1] + 2)\n",
    "        if retrieved_word_str[0].strip() == original_tkn.strip():\n",
    "            predicted_layers.append(layer_idx)\n",
    "        predicted_values.append(retrieved_word_str[0].strip())\n",
    "    record['predicted_layers'] = predicted_layers\n",
    "    record['predicted_values'] = predicted_values\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_cnt = filtered_cnt.sort_values('less_frequent_rank', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29411/29411 [29:38:45<00:00,  3.63s/it]   \n"
     ]
    }
   ],
   "source": [
    "all_records = pickle.load(open('data/llama3_filtered_cnt_valid.pkl', 'rb'))\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for idx, row in tqdm(filtered_cnt.iterrows(), total=len(filtered_cnt)):\n",
    "    cnt +=1\n",
    "    if cnt <= len(all_records):\n",
    "        continue\n",
    "    enriched_row = get_token_layers(row)\n",
    "    all_records.append(enriched_row)\n",
    "    if cnt % 10 == 0:\n",
    "        pickle.dump(all_records, open('data/llama3_filtered_cnt_valid.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_records_df = pd.DataFrame(all_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocab_pos</th>\n",
       "      <th>less_frequent_rank</th>\n",
       "      <th>tkn</th>\n",
       "      <th>cnt</th>\n",
       "      <th>unique_but_start_dict_rank</th>\n",
       "      <th>original_tkn</th>\n",
       "      <th>predicted_layers</th>\n",
       "      <th>predicted_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7030</td>\n",
       "      <td>0</td>\n",
       "      <td>××ª×§×™</td>\n",
       "      <td>3</td>\n",
       "      <td>7033</td>\n",
       "      <td>Ä Ã—Å€Ã—ÂªÃ—Â§Ã—Ä»</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ ××ª×§×™,  ××ª×§×™,  ××ª×§×™,  ××ª×§×™,  ××ª×§×™,  ××ª×§×™,  ××ª...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13775</td>\n",
       "      <td>1</td>\n",
       "      <td>××¤×’× </td>\n",
       "      <td>3</td>\n",
       "      <td>13778</td>\n",
       "      <td>Ä Ã—Ä²Ã—Â¤Ã—Ä´Ã—Å‚</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ ××¤×’× ,  ××¤×’× ,  ××¤×’× ,  ××¤×’× ,  ××¤×’× ,  ××¤×’× ,  ××¤...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7818</td>\n",
       "      <td>3</td>\n",
       "      <td>×©×”×ª×§×™</td>\n",
       "      <td>6</td>\n",
       "      <td>7824</td>\n",
       "      <td>Ä Ã—Â©Ã—Ä¶Ã—ÂªÃ—Â§Ã—Ä»</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ ×©×”×ª×§×™,  ×©×”×ª×§×™,  ×©×”×ª×§×™,  ×©×”×ª×§×™,  ×©×”×ª×§×™,  ×©×”×ª×§...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16316</td>\n",
       "      <td>5</td>\n",
       "      <td>××™××¦</td>\n",
       "      <td>6</td>\n",
       "      <td>16322</td>\n",
       "      <td>Ä Ã—Ä²Ã—Ä»Ã—Å€Ã—Â¦</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ ××™××¦,  ××™××¦,  ××™××¦,  ××™××¦,  ××™××¦,  ××™××¦,  ××™...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13708</td>\n",
       "      <td>6</td>\n",
       "      <td>×¨×¤×•×</td>\n",
       "      <td>9</td>\n",
       "      <td>13717</td>\n",
       "      <td>Ä Ã—Â¨Ã—Â¤Ã—Ä·Ã—Ä²</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ ×¨×¤×•×,  ×¨×¤×•×,  ×¨×¤×•×,  ×¨×¤×•×,  ×¨×¤×•×,  ×¨×¤×•×,  ×¨×¤...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29406</th>\n",
       "      <td>131</td>\n",
       "      <td>29511</td>\n",
       "      <td>=</td>\n",
       "      <td>1978188</td>\n",
       "      <td>1978319</td>\n",
       "      <td>Ä =</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ =,  =,  =,  =,  =,  =,  =,  =,  =,  =,  =,  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29407</th>\n",
       "      <td>132</td>\n",
       "      <td>29512</td>\n",
       "      <td>(</td>\n",
       "      <td>2141322</td>\n",
       "      <td>2141454</td>\n",
       "      <td>Ä (</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ (,  (,  (,  (,  (,  (,  (,  (,  (,  (,  (,  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29408</th>\n",
       "      <td>109</td>\n",
       "      <td>29513</td>\n",
       "      <td>×©×œ</td>\n",
       "      <td>3153417</td>\n",
       "      <td>3153526</td>\n",
       "      <td>Ä Ã—Â©Ã—Ä¾</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ ×©×œ,  ×©×œ,  ×©×œ,  ×©×œ,  ×©×œ,  ×©×œ,  ×©×œ,  ×©×œ,  ×©×œ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29409</th>\n",
       "      <td>10</td>\n",
       "      <td>29514</td>\n",
       "      <td>\\n</td>\n",
       "      <td>4449807</td>\n",
       "      <td>4449817</td>\n",
       "      <td>ÄŠ</td>\n",
       "      <td>[]</td>\n",
       "      <td>[\\n, \\n, \\n, \\n, \\n, \\n, \\n, \\n, \\n, \\n, \\n, \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29410</th>\n",
       "      <td>32</td>\n",
       "      <td>29515</td>\n",
       "      <td></td>\n",
       "      <td>6462111</td>\n",
       "      <td>6462143</td>\n",
       "      <td>Ä </td>\n",
       "      <td>[]</td>\n",
       "      <td>[ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29411 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       vocab_pos  less_frequent_rank     tkn      cnt  \\\n",
       "0           7030                   0    ××ª×§×™        3   \n",
       "1          13775                   1    ××¤×’×         3   \n",
       "2           7818                   3   ×©×”×ª×§×™        6   \n",
       "3          16316                   5    ××™××¦        6   \n",
       "4          13708                   6    ×¨×¤×•×        9   \n",
       "...          ...                 ...     ...      ...   \n",
       "29406        131               29511       =  1978188   \n",
       "29407        132               29512       (  2141322   \n",
       "29408        109               29513      ×©×œ  3153417   \n",
       "29409         10               29514      \\n  4449807   \n",
       "29410         32               29515          6462111   \n",
       "\n",
       "       unique_but_start_dict_rank original_tkn predicted_layers  \\\n",
       "0                            7033    Ä Ã—Å€Ã—ÂªÃ—Â§Ã—Ä»               []   \n",
       "1                           13778    Ä Ã—Ä²Ã—Â¤Ã—Ä´Ã—Å‚               []   \n",
       "2                            7824  Ä Ã—Â©Ã—Ä¶Ã—ÂªÃ—Â§Ã—Ä»               []   \n",
       "3                           16322    Ä Ã—Ä²Ã—Ä»Ã—Å€Ã—Â¦               []   \n",
       "4                           13717    Ä Ã—Â¨Ã—Â¤Ã—Ä·Ã—Ä²               []   \n",
       "...                           ...          ...              ...   \n",
       "29406                     1978319           Ä =               []   \n",
       "29407                     2141454           Ä (               []   \n",
       "29408                     3153526        Ä Ã—Â©Ã—Ä¾               []   \n",
       "29409                     4449817            ÄŠ               []   \n",
       "29410                     6462143            Ä                []   \n",
       "\n",
       "                                        predicted_values  \n",
       "0      [ ××ª×§×™,  ××ª×§×™,  ××ª×§×™,  ××ª×§×™,  ××ª×§×™,  ××ª×§×™,  ××ª...  \n",
       "1      [ ××¤×’× ,  ××¤×’× ,  ××¤×’× ,  ××¤×’× ,  ××¤×’× ,  ××¤×’× ,  ××¤...  \n",
       "2      [ ×©×”×ª×§×™,  ×©×”×ª×§×™,  ×©×”×ª×§×™,  ×©×”×ª×§×™,  ×©×”×ª×§×™,  ×©×”×ª×§...  \n",
       "3      [ ××™××¦,  ××™××¦,  ××™××¦,  ××™××¦,  ××™××¦,  ××™××¦,  ××™...  \n",
       "4      [ ×¨×¤×•×,  ×¨×¤×•×,  ×¨×¤×•×,  ×¨×¤×•×,  ×¨×¤×•×,  ×¨×¤×•×,  ×¨×¤...  \n",
       "...                                                  ...  \n",
       "29406  [ =,  =,  =,  =,  =,  =,  =,  =,  =,  =,  =,  ...  \n",
       "29407  [ (,  (,  (,  (,  (,  (,  (,  (,  (,  (,  (,  ...  \n",
       "29408  [ ×©×œ,  ×©×œ,  ×©×œ,  ×©×œ,  ×©×œ,  ×©×œ,  ×©×œ,  ×©×œ,  ×©×œ, ...  \n",
       "29409  [\\n, \\n, \\n, \\n, \\n, \\n, \\n, \\n, \\n, \\n, \\n, \\...  \n",
       "29410  [ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ...  \n",
       "\n",
       "[29411 rows x 8 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_records_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'××ª×§×™'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_vocab = {v:k for k,v in new_tokenizer_30k.vocab.items()}\n",
    "new_tokenizer_30k.decode(new_tokenizer_30k.vocab['Ä Ã—Å€Ã—ÂªÃ—Â§Ã—Ä»']).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™',\n",
       " ' ××ª×§×™']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_records_df.iloc[0]['predicted_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['headers',\n",
       " 'iful',\n",
       " 'BSC',\n",
       " '.gs',\n",
       " 'layout',\n",
       " 'suggest',\n",
       " 'collapse',\n",
       " 'BAD',\n",
       " 'TRL',\n",
       " 'operator',\n",
       " 'required',\n",
       " 'EDA',\n",
       " 'ublisher',\n",
       " 'icros',\n",
       " 'spacing',\n",
       " 'attern',\n",
       " 'Db',\n",
       " 'uggest',\n",
       " 'ilon',\n",
       " 'adding',\n",
       " 'iforn',\n",
       " 'quired',\n",
       " '.blog',\n",
       " 'ACK',\n",
       " '_-',\n",
       " '.tele',\n",
       " '.ny',\n",
       " 'spot',\n",
       " 'sq',\n",
       " 'dead',\n",
       " 'Õ¡',\n",
       " '-work',\n",
       " 'Marker',\n",
       " 'true',\n",
       " '/upload',\n",
       " 'dep',\n",
       " 'alem',\n",
       " 'lymp',\n",
       " 'ware',\n",
       " 'otify',\n",
       " 'fault',\n",
       " 'acebook',\n",
       " 'peg',\n",
       " 'latin',\n",
       " 'ecause',\n",
       " 'ASA',\n",
       " 'agram',\n",
       " 'hebrew',\n",
       " 'toc',\n",
       " 'cele',\n",
       " 'apps',\n",
       " 'iness',\n",
       " 'reference',\n",
       " 'cto',\n",
       " '.app',\n",
       " '.city',\n",
       " 'ample',\n",
       " 'outube',\n",
       " 'zet',\n",
       " 'heel',\n",
       " 'xford',\n",
       " 'Off',\n",
       " 'andom',\n",
       " 'depend',\n",
       " 'eca',\n",
       " 'return',\n",
       " 'ipes',\n",
       " 'HF',\n",
       " 'rys',\n",
       " 'ilit',\n",
       " 'ampion',\n",
       " 'bum',\n",
       " 'irthday',\n",
       " 'region',\n",
       " 'ictures',\n",
       " 'crip',\n",
       " 'ictionary',\n",
       " 'ircle',\n",
       " 'quote',\n",
       " 'fn',\n",
       " 'BN',\n",
       " '-we',\n",
       " '/play',\n",
       " 'volume',\n",
       " 'stitute',\n",
       " 'igma',\n",
       " 'people',\n",
       " 'itation',\n",
       " '×¢×–×¨',\n",
       " '×–×¨×¢',\n",
       " 'ival',\n",
       " 'archives',\n",
       " '.im',\n",
       " 'unction',\n",
       " 'ael',\n",
       " 'onom',\n",
       " 'ground',\n",
       " 'include',\n",
       " 'gypt',\n",
       " 'board',\n",
       " 'Gov',\n",
       " 'apers',\n",
       " 'arge',\n",
       " 'eps',\n",
       " 'location',\n",
       " 'ega',\n",
       " 'cogs',\n",
       " 'assy',\n",
       " '.dis',\n",
       " 'ervice',\n",
       " 'Arch',\n",
       " 'option',\n",
       " 'Wiki',\n",
       " 'cis',\n",
       " 'reate',\n",
       " ':s',\n",
       " 'VD',\n",
       " 'ema',\n",
       " 'rive',\n",
       " 'nal',\n",
       " 'BE',\n",
       " '937',\n",
       " 'wig',\n",
       " 'CP',\n",
       " 'etting',\n",
       " 'aves',\n",
       " 'Ğ¾Ğ±',\n",
       " '/The',\n",
       " 'ulu',\n",
       " '038',\n",
       " '×’×¨×©',\n",
       " 'abel',\n",
       " '945',\n",
       " '798',\n",
       " '938',\n",
       " 'ullet',\n",
       " 'population',\n",
       " 'irk',\n",
       " 'uropean',\n",
       " '879',\n",
       " 'Ah',\n",
       " 'phys',\n",
       " '-last',\n",
       " 'Ö¿',\n",
       " 'bout',\n",
       " 'Mod',\n",
       " 'udi',\n",
       " '?s',\n",
       " '959',\n",
       " 'cor',\n",
       " 'ÑƒÑ‚',\n",
       " '922',\n",
       " '838',\n",
       " '034',\n",
       " '.news',\n",
       " 'emi',\n",
       " 'uts',\n",
       " 'UG',\n",
       " 'adow',\n",
       " 'ustral',\n",
       " 'Ñ‰',\n",
       " 'rah',\n",
       " 'American',\n",
       " '-review',\n",
       " '×¢×‘×“',\n",
       " 'asp',\n",
       " '.all',\n",
       " 'elm',\n",
       " '836',\n",
       " 'artin',\n",
       " 'rev',\n",
       " 'Wik',\n",
       " 'iped',\n",
       " '.by',\n",
       " '977',\n",
       " '045',\n",
       " 'abe',\n",
       " 'ols',\n",
       " 'PDF',\n",
       " 'equ',\n",
       " 'Ö·×Ÿ',\n",
       " 'Ew',\n",
       " 'ublish',\n",
       " 'imal',\n",
       " 'front',\n",
       " '896',\n",
       " '849',\n",
       " 'pret',\n",
       " 'cup',\n",
       " 'Ø§Ø¯',\n",
       " 'iamond',\n",
       " '082',\n",
       " 'agn',\n",
       " '.H',\n",
       " 'ecret',\n",
       " '936',\n",
       " '/al',\n",
       " '987',\n",
       " '954',\n",
       " 'andy',\n",
       " '779',\n",
       " 'esis',\n",
       " 'rog',\n",
       " 'archiv',\n",
       " 'Item',\n",
       " '-dis',\n",
       " '=yes',\n",
       " '898',\n",
       " 'don',\n",
       " '962',\n",
       " 'vertical',\n",
       " 'ells',\n",
       " 'enberg',\n",
       " 'VA',\n",
       " '-ed',\n",
       " 'ENT',\n",
       " '024',\n",
       " 'fm',\n",
       " '×“×•×‘',\n",
       " '839',\n",
       " 'anth',\n",
       " '994',\n",
       " '829',\n",
       " 'arctica',\n",
       " 'audi',\n",
       " '843',\n",
       " 'loss',\n",
       " '.dk',\n",
       " 'ande',\n",
       " '892',\n",
       " 'amps',\n",
       " '_x',\n",
       " 'een',\n",
       " '782',\n",
       " 'OG',\n",
       " 'SE',\n",
       " 'phen',\n",
       " '968',\n",
       " '047',\n",
       " '995',\n",
       " 'lotte',\n",
       " 'Delta',\n",
       " '793',\n",
       " 'vas',\n",
       " '099',\n",
       " 'works',\n",
       " 'Ğ¸Ñ…',\n",
       " '797',\n",
       " 'Omega',\n",
       " 'overn',\n",
       " '739',\n",
       " 'Archive',\n",
       " '884',\n",
       " '877',\n",
       " '.is',\n",
       " 'fd',\n",
       " '915',\n",
       " '932',\n",
       " 'Ğ‘',\n",
       " 'dom',\n",
       " '955',\n",
       " '035',\n",
       " '799',\n",
       " '927',\n",
       " 'atz',\n",
       " '871',\n",
       " '.eu',\n",
       " 'flo',\n",
       " '873',\n",
       " '784',\n",
       " 'erve',\n",
       " 'aim',\n",
       " 'EED',\n",
       " 'VO',\n",
       " 'hor',\n",
       " '963',\n",
       " '.go',\n",
       " '_news',\n",
       " 'ovo',\n",
       " 'display',\n",
       " ')(',\n",
       " '874',\n",
       " '-str',\n",
       " '942',\n",
       " 'utter',\n",
       " 'Â°C',\n",
       " 'dr',\n",
       " 'hp',\n",
       " '934',\n",
       " 'blockquote',\n",
       " 'andra',\n",
       " '929',\n",
       " '762',\n",
       " ':g',\n",
       " 'uri',\n",
       " 'ÙŠØ±',\n",
       " 'bes',\n",
       " 'olymp',\n",
       " '/photo',\n",
       " 'ources',\n",
       " '895',\n",
       " '×–××ª',\n",
       " 'cu',\n",
       " 'What',\n",
       " '985',\n",
       " '687',\n",
       " 'Å',\n",
       " 'GM',\n",
       " '923',\n",
       " '756',\n",
       " 'razil',\n",
       " 'sta',\n",
       " '991',\n",
       " 'ION',\n",
       " '_r',\n",
       " '969',\n",
       " '893',\n",
       " 'ledge',\n",
       " 'options',\n",
       " 'NE',\n",
       " '842',\n",
       " 'ube',\n",
       " 'zog',\n",
       " 'cho',\n",
       " 'ired',\n",
       " 'Ù„ÙŠ',\n",
       " '/chart',\n",
       " 'iu',\n",
       " 'ning',\n",
       " '&M',\n",
       " 'frame',\n",
       " '948',\n",
       " 'aux',\n",
       " '-series',\n",
       " 'AK',\n",
       " 'ME',\n",
       " 'Type',\n",
       " 'appen',\n",
       " '881',\n",
       " '869',\n",
       " 'SP',\n",
       " 'volution',\n",
       " 'ification',\n",
       " '.tr',\n",
       " '822',\n",
       " '676',\n",
       " 'unes',\n",
       " 'erk',\n",
       " '914',\n",
       " 'more',\n",
       " 'asia',\n",
       " 'vy',\n",
       " 'so',\n",
       " 'otor',\n",
       " 'atas',\n",
       " 'helm',\n",
       " 'atre',\n",
       " '.P',\n",
       " 'opp',\n",
       " '692',\n",
       " '961',\n",
       " '-ar',\n",
       " 'ervices',\n",
       " 'ding',\n",
       " '-rel',\n",
       " 'urrent',\n",
       " 'Coord',\n",
       " 'tro',\n",
       " 'UP',\n",
       " '837',\n",
       " 'cription',\n",
       " 'Ã­n',\n",
       " 'agy',\n",
       " 'active',\n",
       " '988',\n",
       " '688',\n",
       " '×¤×¨×—',\n",
       " 'Me',\n",
       " '.id',\n",
       " 'rin',\n",
       " '=true',\n",
       " '971',\n",
       " 'lit',\n",
       " '854',\n",
       " 'oftware',\n",
       " '785',\n",
       " 'asket',\n",
       " 'sf',\n",
       " 'refer',\n",
       " '686',\n",
       " 'Br',\n",
       " '/people',\n",
       " '925',\n",
       " '861',\n",
       " '=a',\n",
       " 'John',\n",
       " 'athan',\n",
       " ',b',\n",
       " '841',\n",
       " ':b',\n",
       " '862',\n",
       " 'oor',\n",
       " '882',\n",
       " '738',\n",
       " 'illage',\n",
       " '759',\n",
       " '983',\n",
       " '848',\n",
       " 'tele',\n",
       " 'entral',\n",
       " 'amin',\n",
       " 'Car',\n",
       " '×¦×',\n",
       " '796',\n",
       " 'HE',\n",
       " 'engine',\n",
       " 'alen',\n",
       " 'citation',\n",
       " 'well',\n",
       " '718',\n",
       " '732',\n",
       " '817',\n",
       " 'Ele',\n",
       " '772',\n",
       " 'Ã¨re',\n",
       " 'itions',\n",
       " 'AQ',\n",
       " 'NS',\n",
       " 'ynam',\n",
       " 'icro',\n",
       " 'eq',\n",
       " '.nz',\n",
       " 'quire',\n",
       " 'ived',\n",
       " '083',\n",
       " 'ocaust',\n",
       " 'rb',\n",
       " '-art',\n",
       " '072',\n",
       " 'late',\n",
       " '.ed',\n",
       " '868',\n",
       " 'uke',\n",
       " 'Ğ°Ğº',\n",
       " '679',\n",
       " 'esch',\n",
       " '819',\n",
       " 'rait',\n",
       " 'this',\n",
       " 'agement',\n",
       " 'artment',\n",
       " '049',\n",
       " 'uide',\n",
       " '867',\n",
       " 'uba',\n",
       " 'jan',\n",
       " 'elo',\n",
       " 'ieg',\n",
       " '825',\n",
       " '958',\n",
       " '697',\n",
       " 'iran',\n",
       " 'amar',\n",
       " '864',\n",
       " 'show',\n",
       " '951',\n",
       " 'hu',\n",
       " '\\\\cdot',\n",
       " '886',\n",
       " '851',\n",
       " 'arker',\n",
       " 'ige',\n",
       " '921',\n",
       " '-or',\n",
       " '993',\n",
       " '713',\n",
       " '018',\n",
       " '748',\n",
       " 'lain',\n",
       " '674',\n",
       " '037',\n",
       " '823',\n",
       " 'alse',\n",
       " '827',\n",
       " 'orks',\n",
       " 'eden',\n",
       " 'rc',\n",
       " 'href',\n",
       " '659',\n",
       " '055',\n",
       " 'round',\n",
       " 'AV',\n",
       " '946',\n",
       " '821',\n",
       " 'rug',\n",
       " 'admin',\n",
       " '878',\n",
       " '872',\n",
       " 'blue',\n",
       " '039',\n",
       " 'avel',\n",
       " '781',\n",
       " '826',\n",
       " 'LE',\n",
       " '831',\n",
       " 'RNA',\n",
       " '813',\n",
       " '956',\n",
       " 'Web',\n",
       " '.u',\n",
       " '792',\n",
       " 'UT',\n",
       " 'ollywood',\n",
       " '916',\n",
       " '729',\n",
       " 'omer',\n",
       " '899',\n",
       " '919',\n",
       " '795',\n",
       " '042',\n",
       " 'cap',\n",
       " 'quest',\n",
       " 'wg',\n",
       " 'stor',\n",
       " '648',\n",
       " '744',\n",
       " '939',\n",
       " 'rome',\n",
       " 'Official',\n",
       " ':black',\n",
       " 'Pe',\n",
       " '885',\n",
       " '930',\n",
       " '941',\n",
       " '>O',\n",
       " '931',\n",
       " '-car',\n",
       " 'otten',\n",
       " 'cord',\n",
       " 'rep',\n",
       " 'lines',\n",
       " 'imag',\n",
       " 'lov',\n",
       " 'history',\n",
       " 'east',\n",
       " '773',\n",
       " '××œ×£',\n",
       " 'Ã¡s',\n",
       " '647',\n",
       " '695',\n",
       " '846',\n",
       " '786',\n",
       " 'Ğ¾Ğ¿',\n",
       " '-ac',\n",
       " 'estival',\n",
       " '678',\n",
       " '677',\n",
       " 'lick',\n",
       " 'duc',\n",
       " 'aat',\n",
       " 'cb',\n",
       " 'iction',\n",
       " 'mag',\n",
       " 'oci',\n",
       " '967',\n",
       " '852',\n",
       " '_ch',\n",
       " 'tries',\n",
       " 'zo',\n",
       " 'elli',\n",
       " '724',\n",
       " 'inf',\n",
       " '788',\n",
       " 'Ã¡r',\n",
       " '668',\n",
       " 'mi',\n",
       " 'ends',\n",
       " '××‘×¨',\n",
       " '845',\n",
       " 'odge',\n",
       " '&nbsp',\n",
       " 'oker',\n",
       " 'erse',\n",
       " 'Ø£',\n",
       " 'Que',\n",
       " 'ao',\n",
       " '844',\n",
       " 'ono',\n",
       " '081',\n",
       " 'blog',\n",
       " 'elig',\n",
       " 'chool',\n",
       " 'heim',\n",
       " 'aza',\n",
       " '649',\n",
       " 'This',\n",
       " '-off',\n",
       " 'atab',\n",
       " 'afe',\n",
       " 'ocial',\n",
       " 'gent',\n",
       " '689',\n",
       " '723',\n",
       " '814',\n",
       " 'HA',\n",
       " 'block',\n",
       " ':m',\n",
       " 'alis',\n",
       " 'cher',\n",
       " '.un',\n",
       " '764',\n",
       " 'Ge',\n",
       " 'size',\n",
       " 'uma',\n",
       " '787',\n",
       " '774',\n",
       " 'nu',\n",
       " '743',\n",
       " 'XX',\n",
       " '753',\n",
       " 'ici',\n",
       " 'oge',\n",
       " '-te',\n",
       " '726',\n",
       " 'ami',\n",
       " '883',\n",
       " '673',\n",
       " 'rot',\n",
       " '857',\n",
       " '863',\n",
       " '935',\n",
       " 'CCCC',\n",
       " '-db',\n",
       " 'Ù‰',\n",
       " '745',\n",
       " '651',\n",
       " '767',\n",
       " 'raph',\n",
       " '865',\n",
       " '794',\n",
       " '032',\n",
       " '917',\n",
       " 'olo',\n",
       " 'SR',\n",
       " '719',\n",
       " 'ball',\n",
       " '015',\n",
       " 'lete',\n",
       " '684',\n",
       " '728',\n",
       " 'BF',\n",
       " 'Can',\n",
       " 'ateg',\n",
       " 'uh',\n",
       " 'edes',\n",
       " 'RD',\n",
       " '791',\n",
       " '643',\n",
       " '859',\n",
       " '966',\n",
       " 'raham',\n",
       " 'mall',\n",
       " '758',\n",
       " 'circle',\n",
       " 'img',\n",
       " '.ua',\n",
       " 'à¤¾',\n",
       " '757',\n",
       " '835',\n",
       " 'HC',\n",
       " 'oda',\n",
       " '754',\n",
       " '559',\n",
       " '980',\n",
       " 'ownload',\n",
       " 'rish',\n",
       " 'AG',\n",
       " '694',\n",
       " 'awn',\n",
       " '×¢×‘×¨',\n",
       " 'mm',\n",
       " '693',\n",
       " 'SM',\n",
       " 'IK',\n",
       " 'iber',\n",
       " '644',\n",
       " '.le',\n",
       " 'hit',\n",
       " 'abeth',\n",
       " '/co',\n",
       " '824',\n",
       " '672',\n",
       " '691',\n",
       " 'ports',\n",
       " '789',\n",
       " '858',\n",
       " '/fr',\n",
       " '742',\n",
       " 'label',\n",
       " 'irit',\n",
       " '061',\n",
       " '776',\n",
       " '818',\n",
       " '661',\n",
       " '736',\n",
       " 'Ph',\n",
       " 'frica',\n",
       " 'ype',\n",
       " 'etail',\n",
       " 'photo',\n",
       " '751',\n",
       " '755',\n",
       " '828',\n",
       " 'SD',\n",
       " 'new',\n",
       " 'GC',\n",
       " '847',\n",
       " 'oogle',\n",
       " '698',\n",
       " '746',\n",
       " 'formation',\n",
       " 'bin',\n",
       " 'room',\n",
       " 'table',\n",
       " '_de',\n",
       " '771',\n",
       " '.ne',\n",
       " '646',\n",
       " 'arta',\n",
       " '638',\n",
       " 'NN',\n",
       " 'lap',\n",
       " 'Ğ¾Ğ³Ğ¾',\n",
       " '629',\n",
       " 'PM',\n",
       " '-au',\n",
       " 'itle',\n",
       " 'tails',\n",
       " 'ravel',\n",
       " '-ab',\n",
       " '>H',\n",
       " 'ough',\n",
       " '761',\n",
       " '-pr',\n",
       " 'OW',\n",
       " '675',\n",
       " '025',\n",
       " 'election',\n",
       " '587',\n",
       " '028',\n",
       " '557',\n",
       " '=k',\n",
       " '598',\n",
       " '614',\n",
       " 'vid',\n",
       " '_st',\n",
       " '091',\n",
       " 'ÑĞºĞ°Ñ',\n",
       " 'alue',\n",
       " '/display',\n",
       " 'vin',\n",
       " '721',\n",
       " 'Black',\n",
       " 'Ã­a',\n",
       " '763',\n",
       " 'ito',\n",
       " 'nder',\n",
       " '741',\n",
       " 'SL',\n",
       " '768',\n",
       " '769',\n",
       " 'urope',\n",
       " 'eni',\n",
       " 'Ñ€Ñƒ',\n",
       " 'nes',\n",
       " 'lip',\n",
       " 'elta',\n",
       " 'utt',\n",
       " '875',\n",
       " 'inners',\n",
       " '027',\n",
       " 'Art',\n",
       " 'Tr',\n",
       " 'EO',\n",
       " '653',\n",
       " '731',\n",
       " 'anti',\n",
       " '671',\n",
       " '.wordpress',\n",
       " 'arrow',\n",
       " '_title',\n",
       " 'ores',\n",
       " '××‘×',\n",
       " 'É™',\n",
       " 'pattern',\n",
       " 'itte',\n",
       " 'alo',\n",
       " '597',\n",
       " 'ache',\n",
       " '052',\n",
       " '887',\n",
       " '766',\n",
       " '816',\n",
       " 'Ä‡',\n",
       " 'error',\n",
       " '735',\n",
       " 'Mar',\n",
       " 'itunes',\n",
       " '855',\n",
       " '685',\n",
       " 'media',\n",
       " 'aus',\n",
       " 'merican',\n",
       " 'aba',\n",
       " 'zen',\n",
       " 'arp',\n",
       " '933',\n",
       " 'Î¿Ï‚',\n",
       " 'via',\n",
       " '918',\n",
       " '696',\n",
       " 'orf',\n",
       " '573',\n",
       " '579',\n",
       " '727',\n",
       " ':red',\n",
       " 'stone',\n",
       " 'rho',\n",
       " '×—×§×¨',\n",
       " '775',\n",
       " 'iny',\n",
       " 'adi',\n",
       " '641',\n",
       " 'fall',\n",
       " '-Ch',\n",
       " 'base',\n",
       " '624',\n",
       " 'ills',\n",
       " '656',\n",
       " '.ge',\n",
       " 'OH',\n",
       " 'Ö±',\n",
       " '681',\n",
       " 'default',\n",
       " '×˜×•×‘',\n",
       " 'utch',\n",
       " '538',\n",
       " '628',\n",
       " '619',\n",
       " '048',\n",
       " 'tos',\n",
       " '657',\n",
       " '866',\n",
       " '749',\n",
       " 'olk',\n",
       " '815',\n",
       " 'ault',\n",
       " '-am',\n",
       " '/se',\n",
       " '626',\n",
       " 'gan',\n",
       " '552',\n",
       " 'ortal',\n",
       " 'ando',\n",
       " 'FM',\n",
       " 'TF',\n",
       " '036',\n",
       " '564',\n",
       " 'Time',\n",
       " 'PC',\n",
       " '/new',\n",
       " '595',\n",
       " '940',\n",
       " 'rix',\n",
       " '026',\n",
       " 'ordan',\n",
       " '×“× ',\n",
       " 'RC',\n",
       " 'xx',\n",
       " 'ple',\n",
       " '635',\n",
       " 'uar',\n",
       " 'Co',\n",
       " 'ouch',\n",
       " 'itzer',\n",
       " '023',\n",
       " 'ÙˆØ±',\n",
       " 'Ø§Ù…',\n",
       " 'elect',\n",
       " 'EG',\n",
       " '645',\n",
       " 'dam',\n",
       " '-group',\n",
       " 'subset',\n",
       " 'sy',\n",
       " '>R',\n",
       " 'iron',\n",
       " 'rama',\n",
       " 'png',\n",
       " '574',\n",
       " 'ices',\n",
       " 'utsch',\n",
       " '=h',\n",
       " '699',\n",
       " 'oft',\n",
       " 'oyal',\n",
       " '599',\n",
       " 'ottom',\n",
       " 'epsilon',\n",
       " '-width',\n",
       " 'ori',\n",
       " 'BI',\n",
       " 'cro',\n",
       " 'aro',\n",
       " '-ne',\n",
       " '783',\n",
       " '-St',\n",
       " 'deadline',\n",
       " 'ART',\n",
       " '715',\n",
       " '664',\n",
       " 'irus',\n",
       " 'but',\n",
       " '584',\n",
       " 'ummer',\n",
       " '016',\n",
       " 'ules',\n",
       " 'set',\n",
       " 'raine',\n",
       " '752',\n",
       " '/docs',\n",
       " '725',\n",
       " '/book',\n",
       " 'Ğ¾Ğ³',\n",
       " '617',\n",
       " 'riend',\n",
       " '658',\n",
       " '716',\n",
       " '596',\n",
       " 'onia',\n",
       " 'achine',\n",
       " 'match',\n",
       " 'tz',\n",
       " '765',\n",
       " '665',\n",
       " '/home',\n",
       " 'asil',\n",
       " 'ione',\n",
       " '593',\n",
       " 'istr',\n",
       " '683',\n",
       " 'akh',\n",
       " '014',\n",
       " '.B',\n",
       " 'leases',\n",
       " 'chi',\n",
       " '>G',\n",
       " '594',\n",
       " '536',\n",
       " '.am',\n",
       " '.E',\n",
       " '.M',\n",
       " 'ING',\n",
       " \"'A\",\n",
       " '518',\n",
       " '623',\n",
       " '663',\n",
       " 'spect',\n",
       " 'fs',\n",
       " 'RIAA',\n",
       " 'asa',\n",
       " 'ifornia',\n",
       " 'ussia',\n",
       " 'isk',\n",
       " '636',\n",
       " '618',\n",
       " 'unity',\n",
       " 'aval',\n",
       " '542',\n",
       " 'ouble',\n",
       " 'reek',\n",
       " 'aylor',\n",
       " 'URL',\n",
       " '637',\n",
       " 'do',\n",
       " 'ete',\n",
       " '655',\n",
       " '627',\n",
       " 'Sch',\n",
       " 'occer',\n",
       " '/post',\n",
       " '561',\n",
       " 'gg',\n",
       " 'trans',\n",
       " 'only',\n",
       " 'layer',\n",
       " 'IAA',\n",
       " 'lood',\n",
       " 'Tube',\n",
       " '567',\n",
       " ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_records_df[all_records_df['predicted_layers'].str.len() != 0]['tkn'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|â–ˆâ–ˆâ–ˆâ–‹      | 10915/29411 [8:44:21<16:23:27,  3.19s/it]"
     ]
    }
   ],
   "source": [
    "# filtered_cnt = filtered_cnt.progress_apply(get_token_layers, axis=1)\n",
    "\n",
    "# pickle.dump(filtered_cnt, open('data/llama3_filtered_cnt.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

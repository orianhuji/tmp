{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 0 examples [00:00, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting content from /home/orian/.cache/huggingface/datasets/downloads/c30d1c3fc72df68ffab064438c260799f73db11cdf4eeb1e48d431bc73ce07a6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 0 examples [00:01, ? examples/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[0;32m----> 2\u001b[0m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwikipedia\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhe\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m20241201\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/PycharmProjects/Tokens2Words/.venv/lib/python3.10/site-packages/datasets/load.py:2154\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[1;32m   2153\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2154\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2155\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2156\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2157\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2158\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2159\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2160\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2162\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2163\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2164\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2165\u001b[0m )\n",
      "File \u001b[0;32m~/PycharmProjects/Tokens2Words/.venv/lib/python3.10/site-packages/datasets/builder.py:924\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    923\u001b[0m     prepare_split_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_proc\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_proc\n\u001b[0;32m--> 924\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    925\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    926\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    927\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    928\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    929\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[1;32m    931\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(split\u001b[38;5;241m.\u001b[39mnum_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39msplits\u001b[38;5;241m.\u001b[39mvalues())\n",
      "File \u001b[0;32m~/PycharmProjects/Tokens2Words/.venv/lib/python3.10/site-packages/datasets/builder.py:1000\u001b[0m, in \u001b[0;36mDatasetBuilder._download_and_prepare\u001b[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    996\u001b[0m split_dict\u001b[38;5;241m.\u001b[39madd(split_generator\u001b[38;5;241m.\u001b[39msplit_info)\n\u001b[1;32m    998\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    999\u001b[0m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[0;32m-> 1000\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1001\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1002\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[1;32m   1003\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot find data file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1004\u001b[0m         \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1005\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1006\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[1;32m   1007\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/PycharmProjects/Tokens2Words/.venv/lib/python3.10/site-packages/datasets/builder.py:1741\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split\u001b[0;34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[0m\n\u001b[1;32m   1739\u001b[0m job_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m   1740\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[0;32m-> 1741\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m job_id, done, content \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_split_single(\n\u001b[1;32m   1742\u001b[0m         gen_kwargs\u001b[38;5;241m=\u001b[39mgen_kwargs, job_id\u001b[38;5;241m=\u001b[39mjob_id, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m_prepare_split_args\n\u001b[1;32m   1743\u001b[0m     ):\n\u001b[1;32m   1744\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   1745\u001b[0m             result \u001b[38;5;241m=\u001b[39m content\n",
      "File \u001b[0;32m~/PycharmProjects/Tokens2Words/.venv/lib/python3.10/site-packages/datasets/builder.py:1854\u001b[0m, in \u001b[0;36mArrowBasedBuilder._prepare_split_single\u001b[0;34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1853\u001b[0m     _time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m-> 1854\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, table \u001b[38;5;129;01min\u001b[39;00m generator:\n\u001b[1;32m   1855\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m max_shard_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m writer\u001b[38;5;241m.\u001b[39m_num_bytes \u001b[38;5;241m>\u001b[39m max_shard_size:\n\u001b[1;32m   1856\u001b[0m             num_examples, num_bytes \u001b[38;5;241m=\u001b[39m writer\u001b[38;5;241m.\u001b[39mfinalize()\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/wikipedia/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia.py:1018\u001b[0m, in \u001b[0;36mWikipedia._generate_tables\u001b[0;34m(self, files, is_processed)\u001b[0m\n\u001b[1;32m   1016\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files:\n\u001b[1;32m   1017\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgenerating examples from = \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, file)\n\u001b[0;32m-> 1018\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m content \u001b[38;5;129;01min\u001b[39;00m _extract_content(file):\n\u001b[1;32m   1019\u001b[0m         example \u001b[38;5;241m=\u001b[39m _clean_content(content, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlanguage)\n\u001b[1;32m   1020\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m example \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/wikipedia/d41137e149b2ea90eead07e7e3f805119a8c22dd1d5b61651af8e3e3ee736001/wikipedia.py:1042\u001b[0m, in \u001b[0;36m_extract_content\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m   1040\u001b[0m utf_f \u001b[38;5;241m=\u001b[39m codecs\u001b[38;5;241m.\u001b[39mgetreader(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m)(f)\n\u001b[1;32m   1041\u001b[0m context \u001b[38;5;241m=\u001b[39m etree\u001b[38;5;241m.\u001b[39miterparse(utf_f, events\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mend\u001b[39m\u001b[38;5;124m\"\u001b[39m,))\n\u001b[0;32m-> 1042\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m unused_event, elem \u001b[38;5;129;01min\u001b[39;00m context:\n\u001b[1;32m   1043\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m elem\u001b[38;5;241m.\u001b[39mtag\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpage\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/xml/etree/ElementTree.py:1255\u001b[0m, in \u001b[0;36miterparse.<locals>.iterator\u001b[0;34m(source)\u001b[0m\n\u001b[1;32m   1253\u001b[0m \u001b[38;5;28;01myield from\u001b[39;00m pullparser\u001b[38;5;241m.\u001b[39mread_events()\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;66;03m# load event buffer\u001b[39;00m\n\u001b[0;32m-> 1255\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43msource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1256\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m data:\n\u001b[1;32m   1257\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/codecs.py:498\u001b[0m, in \u001b[0;36mStreamReader.read\u001b[0;34m(self, size, chars, firstline)\u001b[0m\n\u001b[1;32m    496\u001b[0m     newdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 498\u001b[0m     newdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;66;03m# decode bytes (those remaining from the last call included)\u001b[39;00m\n\u001b[1;32m    500\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbytebuffer \u001b[38;5;241m+\u001b[39m newdata\n",
      "File \u001b[0;32m/usr/lib/python3.10/bz2.py:164\u001b[0m, in \u001b[0;36mBZ2File.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Read up to size uncompressed bytes from the file.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03mIf size is negative or omitted, read until EOF is reached.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;03mReturns b'' if the file is already at EOF.\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_can_read()\n\u001b[0;32m--> 164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/_compression.py:68\u001b[0m, in \u001b[0;36mDecompressReader.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreadinto\u001b[39m(\u001b[38;5;28mself\u001b[39m, b):\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mmemoryview\u001b[39m(b) \u001b[38;5;28;01mas\u001b[39;00m view, view\u001b[38;5;241m.\u001b[39mcast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m byte_view:\n\u001b[0;32m---> 68\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbyte_view\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m         byte_view[:\u001b[38;5;28mlen\u001b[39m(data)] \u001b[38;5;241m=\u001b[39m data\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data)\n",
      "File \u001b[0;32m/usr/lib/python3.10/_compression.py:103\u001b[0m, in \u001b[0;36mDecompressReader.read\u001b[0;34m(self, size)\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m         rawblock \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 103\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_decompressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecompress\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrawblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m data:\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "load_dataset(\"wikipedia\", language=\"he\", date=\"20241201\", trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import xml.etree.ElementTree as etree\n",
    "\n",
    "\n",
    "pathWikiXML = '/home/orian/PycharmProjects/Tokens2Words/data/hewiki-latest-pages-articles.xml'\n",
    "\n",
    "\n",
    "def strip_tag_name(t):\n",
    "    return t.split(\"}\")[1] if \"}\" in t else t\n",
    "\n",
    "all_wiki_records = []\n",
    "\n",
    "cnt = 0\n",
    "for event, elem in etree.iterparse(pathWikiXML, events=('start', 'end')):\n",
    "    tname = strip_tag_name(elem.tag)\n",
    "\n",
    "    if event == 'start':\n",
    "        if tname == 'text':\n",
    "            text = elem.text\n",
    "            if text:\n",
    "                text = '\\n'.join([l for l in text.split('\\n') if not (l.startswith('<') or l.startswith('>') or l.startswith('[[') or l.startswith('{{') or l.startswith('__') or l.startswith('==') or l.startswith('*') or l.startswith('#') or l.startswith(\"'''\"))])\n",
    "                text = re.sub(r'\\|.+?\\]\\]', '', text).replace('[[', '').replace(']]', '').replace(\"'''\", '')\n",
    "                text = re.sub(r'\\<ref\\>.+?\\<\\/ref\\>', '', text).strip()\n",
    "\n",
    "                if text:\n",
    "                    all_wiki_records.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'mistralai/Mistral-7B-Instruct-v0.3'\n",
    "model_name = 'Mistral'\n",
    "token = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_tokenizer_100 = tokenizer.train_new_from_iterator(iter(all_wiki_records[:100]), vocab_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "927"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(new_tokenizer_100.vocab.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_tokenizer_30k = tokenizer.train_new_from_iterator(iter(all_wiki_records), vocab_size=30_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['𒂗',\n",
       " '𒂟',\n",
       " '𒃶',\n",
       " '𒄈',\n",
       " '𒄠',\n",
       " '𒄩',\n",
       " '𒅁',\n",
       " '𒅅',\n",
       " '𒅎',\n",
       " '𒆍',\n",
       " '𒆠',\n",
       " '𒆳',\n",
       " '𒈥',\n",
       " '𒈪',\n",
       " '𒈬',\n",
       " '𒈾',\n",
       " '𒉆',\n",
       " '𒉌',\n",
       " '𒊏',\n",
       " '𒊩',\n",
       " '𒋗',\n",
       " '𒋢',\n",
       " '𒋻',\n",
       " '𒋾',\n",
       " '𒌆',\n",
       " '𒌌',\n",
       " '𒎏',\n",
       " '𓁟',\n",
       " '𓂆',\n",
       " '𓂋',\n",
       " '𓂻',\n",
       " '𓄤',\n",
       " '𓅝',\n",
       " '𓆑',\n",
       " '𓇋',\n",
       " '𓇍',\n",
       " '𓈉',\n",
       " '𓈖',\n",
       " '𓉔',\n",
       " '𓊽',\n",
       " '𓍘',\n",
       " '𓏏',\n",
       " '𓏭',\n",
       " '𝄫',\n",
       " '𝐺',\n",
       " '𝑜',\n",
       " '𝒶',\n",
       " '𝒾',\n",
       " '𝓂',\n",
       " '𝓇',\n",
       " '𝓚',\n",
       " '𝓟',\n",
       " '𝓪',\n",
       " '𝓭',\n",
       " '𝓮',\n",
       " '𝓲',\n",
       " '𝓵',\n",
       " '𝓷',\n",
       " '\\U0001df0a',\n",
       " '🇦',\n",
       " '🇧',\n",
       " '🇨',\n",
       " '🇩',\n",
       " '🇪',\n",
       " '🇫',\n",
       " '🇬',\n",
       " '🇭',\n",
       " '🇮',\n",
       " '🇰',\n",
       " '🇱',\n",
       " '🇲',\n",
       " '🇳',\n",
       " '🇴',\n",
       " '🇵',\n",
       " '🇷',\n",
       " '🇸',\n",
       " '🇹',\n",
       " '🇺',\n",
       " '🇻',\n",
       " '🇾',\n",
       " '🇿',\n",
       " '🌈',\n",
       " '🌙',\n",
       " '🌞',\n",
       " '🌫',\n",
       " '🌴',\n",
       " '🌷',\n",
       " '🌸',\n",
       " '🌹',\n",
       " '🌼',\n",
       " '🎂',\n",
       " '🎉',\n",
       " '🎗',\n",
       " '🎤',\n",
       " '🎧',\n",
       " '🎬',\n",
       " '🎶',\n",
       " '🎸',\n",
       " '🎻',\n",
       " '🏆',\n",
       " '🏳',\n",
       " '🏹',\n",
       " '🏻',\n",
       " '🏼',\n",
       " '🐦',\n",
       " '🐾',\n",
       " '👆',\n",
       " '👇',\n",
       " '👋',\n",
       " '👍',\n",
       " '👎',\n",
       " '👻',\n",
       " '👼',\n",
       " '💃',\n",
       " '💋',\n",
       " '💍',\n",
       " '💐',\n",
       " '💑',\n",
       " '💒',\n",
       " '💔',\n",
       " '💕',\n",
       " '💗',\n",
       " '💘',\n",
       " '💙',\n",
       " '💚',\n",
       " '💛',\n",
       " '💜',\n",
       " '💝',\n",
       " '💡',\n",
       " '💩',\n",
       " '💪',\n",
       " '💫',\n",
       " '💭',\n",
       " '💿',\n",
       " '📀',\n",
       " '📒',\n",
       " '📕',\n",
       " '📖',\n",
       " '📸',\n",
       " '🔄',\n",
       " '🔑',\n",
       " '🔥',\n",
       " '🔪',\n",
       " '🕍',\n",
       " '🕵',\n",
       " '🕷',\n",
       " '🖤',\n",
       " '😀',\n",
       " '😁',\n",
       " '😂',\n",
       " '😊',\n",
       " '😍',\n",
       " '😑',\n",
       " '😜',\n",
       " '😳',\n",
       " '🙂',\n",
       " '🙈',\n",
       " '🙏',\n",
       " '🚆',\n",
       " '🚨',\n",
       " '🛡',\n",
       " '🜨',\n",
       " '🤍',\n",
       " '🤎',\n",
       " '🤔',\n",
       " '🤘',\n",
       " '🤦',\n",
       " '🤪',\n",
       " '🤷',\n",
       " '🥂',\n",
       " '🥳',\n",
       " '🥺',\n",
       " '🦋',\n",
       " '🦔',\n",
       " '🧡',\n",
       " '🧨',\n",
       " '𠆯',\n",
       " '𠘨',\n",
       " '𠥓',\n",
       " '𣯶',\n",
       " '𤰓',\n",
       " '𤰔',\n",
       " '𥥍',\n",
       " '▁ה',\n",
       " '▁ב',\n",
       " 'ת▁',\n",
       " 'ה▁',\n",
       " 'ים',\n",
       " '▁מ',\n",
       " '\\n|',\n",
       " '▁ש',\n",
       " 'ור',\n",
       " '▁א',\n",
       " '▁ל',\n",
       " 'ות',\n",
       " 'ני',\n",
       " '▁ו',\n",
       " 'רי',\n",
       " 'לי',\n",
       " 'רו',\n",
       " '}}',\n",
       " 'ל▁',\n",
       " '\\n\\n',\n",
       " 'יו',\n",
       " 'ת▁ה',\n",
       " 'מו',\n",
       " '▁ע',\n",
       " '20',\n",
       " '{{',\n",
       " 'נו',\n",
       " ',▁',\n",
       " 'לו',\n",
       " 'יי',\n",
       " 'קו',\n",
       " 'או',\n",
       " 'די',\n",
       " '▁כ',\n",
       " 'בי',\n",
       " 'ה▁ה',\n",
       " 'יר',\n",
       " '19',\n",
       " 'בר',\n",
       " 'ול',\n",
       " 'מי',\n",
       " 'ה▁ב',\n",
       " 'תו',\n",
       " 'בו',\n",
       " 'ן▁',\n",
       " 'פר',\n",
       " 'אי',\n",
       " 'ל▁ה',\n",
       " '=▁',\n",
       " 'מש',\n",
       " '.\\n\\n',\n",
       " 'דו',\n",
       " 'רא',\n",
       " 'סי',\n",
       " '11',\n",
       " 'ער',\n",
       " 'פי',\n",
       " 'שו',\n",
       " 'ם▁',\n",
       " 'חו',\n",
       " 'קי',\n",
       " 'ים▁',\n",
       " 'חר',\n",
       " 'טי',\n",
       " 'ות▁ה',\n",
       " 'שי',\n",
       " 'ה▁ל',\n",
       " 'וא',\n",
       " 'סו',\n",
       " 'פו',\n",
       " 'שנ',\n",
       " 'אר',\n",
       " '▁י',\n",
       " '▁|',\n",
       " '▁(',\n",
       " 'טו',\n",
       " 'תי',\n",
       " 'ה▁ש',\n",
       " 'חי',\n",
       " '▁בי',\n",
       " '▁במ',\n",
       " '▁המ',\n",
       " '\\n|▁',\n",
       " ',▁ו',\n",
       " 'גו',\n",
       " 'גי',\n",
       " 'er',\n",
       " 'צי',\n",
       " '▁נ',\n",
       " 'הי',\n",
       " 'עו',\n",
       " 'גר',\n",
       " 'דר',\n",
       " 'עי',\n",
       " 'הו',\n",
       " '▁20',\n",
       " 'טר',\n",
       " 'ם▁ה',\n",
       " 'ן▁ה',\n",
       " 'ת▁ש',\n",
       " 'קר',\n",
       " 'כו',\n",
       " '.▁ב',\n",
       " 'נה▁',\n",
       " '▁=▁',\n",
       " '00',\n",
       " 'נה',\n",
       " '▁לא',\n",
       " '.▁ה',\n",
       " '.▁',\n",
       " 'ים▁ב',\n",
       " 'ת▁א',\n",
       " 'ת▁מ',\n",
       " 'or',\n",
       " 'לא',\n",
       " '▁של▁',\n",
       " 'in',\n",
       " 'תר',\n",
       " ',▁ה',\n",
       " '=\"',\n",
       " 'an',\n",
       " '\\n|-',\n",
       " '=\\n|',\n",
       " 'at',\n",
       " 'le',\n",
       " 'הער',\n",
       " 'קור',\n",
       " 'זי',\n",
       " 'ה▁מ',\n",
       " ',▁א',\n",
       " 'חל',\n",
       " 'שר',\n",
       " 'נג',\n",
       " '▁של',\n",
       " 'וי',\n",
       " 'שנת▁',\n",
       " 'צר',\n",
       " 'on',\n",
       " '{{הער',\n",
       " 'נים',\n",
       " 'סט',\n",
       " 'ספר',\n",
       " 'e▁',\n",
       " 'ar',\n",
       " 'תא',\n",
       " 'ברי',\n",
       " 'חד',\n",
       " '▁מו',\n",
       " 'al',\n",
       " 'ת▁ל',\n",
       " ',▁ב',\n",
       " 'מר',\n",
       " 'en',\n",
       " '▁ג',\n",
       " 'ים▁ו',\n",
       " 'כי',\n",
       " 'st',\n",
       " 'ם▁ב',\n",
       " '▁פ',\n",
       " 'חק',\n",
       " '\\n}}',\n",
       " 'ת▁ב',\n",
       " '{{ש',\n",
       " '▁הי',\n",
       " 'ה|',\n",
       " '18',\n",
       " '{{ש}}',\n",
       " 'ספ',\n",
       " 'ה▁א',\n",
       " ',▁ש',\n",
       " '▁של▁ה',\n",
       " '▁לה',\n",
       " 'גל',\n",
       " '▁על▁',\n",
       " '.\\n\\nב',\n",
       " 'ים▁ה',\n",
       " 'ריך',\n",
       " 'ht',\n",
       " 'בע',\n",
       " '▁את▁ה',\n",
       " 'אל',\n",
       " 'צו',\n",
       " '▁או',\n",
       " '▁ק',\n",
       " '\\n\\n|',\n",
       " 'ורי',\n",
       " 'סר',\n",
       " 'co',\n",
       " 'it',\n",
       " '▁\"',\n",
       " 'ותר',\n",
       " 'ות▁ב',\n",
       " 'זו',\n",
       " 'ה▁ו',\n",
       " 're',\n",
       " '▁אי',\n",
       " 'צא',\n",
       " 'נד',\n",
       " 'ן▁ב',\n",
       " '201',\n",
       " 'ראשו',\n",
       " 'אור',\n",
       " 'תאריך',\n",
       " '▁מש',\n",
       " 'ww',\n",
       " 'זר',\n",
       " 'נט',\n",
       " '|▁',\n",
       " '▁למ',\n",
       " 'ת▁ו',\n",
       " '▁הו',\n",
       " 'שם',\n",
       " 'ה▁ע',\n",
       " 'ד▁',\n",
       " '200',\n",
       " '▁את▁',\n",
       " 'נס',\n",
       " 'תב',\n",
       " '{{הערה|',\n",
       " 'תח',\n",
       " 'רב',\n",
       " 'תפ',\n",
       " \"ג'\",\n",
       " 'כל',\n",
       " '▁19',\n",
       " 'דור',\n",
       " ',▁מ',\n",
       " 'מקור',\n",
       " 'שור',\n",
       " 'מא',\n",
       " '▁▁',\n",
       " 'שרא',\n",
       " 'על',\n",
       " 'נית',\n",
       " '▁שי',\n",
       " '▁ח',\n",
       " 'th',\n",
       " 'ים▁מ',\n",
       " 'ידי',\n",
       " ':▁',\n",
       " '//',\n",
       " '▁ס',\n",
       " 'tp',\n",
       " 'מל',\n",
       " 'בור',\n",
       " 's▁',\n",
       " 'http',\n",
       " '://',\n",
       " 'ה=',\n",
       " 'של',\n",
       " 'בא',\n",
       " 'בל',\n",
       " '-1',\n",
       " '▁ז',\n",
       " 'קט',\n",
       " 'פור',\n",
       " 'לד',\n",
       " '1|',\n",
       " '▁וה',\n",
       " 'ה.',\n",
       " 'עיר',\n",
       " 'ן▁מ',\n",
       " 'פרי',\n",
       " 'ניי',\n",
       " '▁לי',\n",
       " 'ותו',\n",
       " '▁ת',\n",
       " 'ת▁ע',\n",
       " 'חבר',\n",
       " '▁ד',\n",
       " 'ig',\n",
       " 'קבו',\n",
       " 'ד▁ה',\n",
       " 'ת▁כ',\n",
       " 'ציר',\n",
       " 'ם▁מ',\n",
       " '||',\n",
       " 'שנת▁19',\n",
       " '\"▁',\n",
       " 'אמ',\n",
       " 'ro',\n",
       " '▁{{ש}}',\n",
       " 'ים▁ש',\n",
       " '\\n!',\n",
       " 'פני',\n",
       " '={{',\n",
       " 'ic',\n",
       " ',▁כ',\n",
       " 'עול',\n",
       " 'מד',\n",
       " 'שפ',\n",
       " '}}\\n|',\n",
       " 'id',\n",
       " 'la',\n",
       " 'ך▁ה',\n",
       " '10',\n",
       " '▁ר',\n",
       " '-19',\n",
       " '▁מי',\n",
       " 'נב',\n",
       " 'שב',\n",
       " 'לח',\n",
       " 'ים▁ל',\n",
       " 'ן▁ל',\n",
       " 'סרט',\n",
       " 'כר',\n",
       " 'es',\n",
       " 'נק',\n",
       " '1111',\n",
       " '▁הוא',\n",
       " 'תמו',\n",
       " 'am',\n",
       " 'ן▁ש',\n",
       " \"צ'\",\n",
       " \"''\",\n",
       " 'פל',\n",
       " 'קד',\n",
       " '12',\n",
       " '▁הש',\n",
       " 'גד',\n",
       " '}}\\n\\n',\n",
       " 'צע',\n",
       " '▁לאחר',\n",
       " 'ליו',\n",
       " 'מבר',\n",
       " 'תפקי',\n",
       " 'שם=',\n",
       " '▁בא',\n",
       " '▁כי',\n",
       " 'יא',\n",
       " 'f▁',\n",
       " 'ל▁א',\n",
       " 'בד',\n",
       " 'לה',\n",
       " '</',\n",
       " 'ac',\n",
       " 'גרס',\n",
       " '22',\n",
       " '17',\n",
       " 'שראל',\n",
       " 'חלק',\n",
       " 'il',\n",
       " 'רכ',\n",
       " 'סק',\n",
       " 'ן▁ו',\n",
       " 'תק',\n",
       " 'קוד',\n",
       " 'ה▁(',\n",
       " 'el',\n",
       " 'ה,▁',\n",
       " '15',\n",
       " 'ל▁מ',\n",
       " 'צות▁ה',\n",
       " 'ה▁של▁',\n",
       " 'שיר',\n",
       " 'ניו',\n",
       " '▁||▁',\n",
       " 'w.',\n",
       " 'יוו',\n",
       " 'ות=',\n",
       " '▁מה',\n",
       " '▁-',\n",
       " '▁יציר',\n",
       " '.\\n\\nה',\n",
       " '▁הא',\n",
       " 'is',\n",
       " 'תוב',\n",
       " 'ent',\n",
       " 'נוס',\n",
       " '16',\n",
       " '23',\n",
       " 'כדור',\n",
       " '\\n|-\\n|',\n",
       " 'הל',\n",
       " '▁אל',\n",
       " '▁באו',\n",
       " 'www.',\n",
       " '13',\n",
       " '▁על▁ידי',\n",
       " 'ותי',\n",
       " '▁שה',\n",
       " 'ur',\n",
       " 'תאריך▁יציר',\n",
       " 'ch',\n",
       " ',▁ע',\n",
       " 'ת▁י',\n",
       " 'צות▁הברי',\n",
       " '.co',\n",
       " '▁אר',\n",
       " 'd▁',\n",
       " '14',\n",
       " '11111|',\n",
       " 'זור',\n",
       " 'ion',\n",
       " 'פע',\n",
       " 'כן',\n",
       " 'עצ',\n",
       " 'קבוצ',\n",
       " 'יית',\n",
       " 'ים.',\n",
       " '▁לו',\n",
       " 'ברו',\n",
       " 'mat',\n",
       " 'פט',\n",
       " 'דע',\n",
       " 'ה▁כ',\n",
       " 'מני',\n",
       " '▁שנ',\n",
       " '\\n}}\\n\\n',\n",
       " 'un',\n",
       " '://www.',\n",
       " '21',\n",
       " 'et',\n",
       " 'אנ',\n",
       " 'פס',\n",
       " '\\n▁',\n",
       " '24',\n",
       " 'נבחר',\n",
       " 'בה',\n",
       " 'דיו',\n",
       " 'מפ',\n",
       " 'קס',\n",
       " 'יוצר',\n",
       " 'ה)',\n",
       " 'מע',\n",
       " 'https',\n",
       " 'ים▁א',\n",
       " 'סוג',\n",
       " 'ה▁=▁',\n",
       " 'sp',\n",
       " 'y▁',\n",
       " '.▁הוא',\n",
       " 'pg',\n",
       " 'ת=',\n",
       " 'מן',\n",
       " '▁עו',\n",
       " 'מנ',\n",
       " 'מקור=',\n",
       " 'צור',\n",
       " 'צב',\n",
       " 'a▁',\n",
       " 'ת▁אחר',\n",
       " '.j',\n",
       " 'נש',\n",
       " '▁בה',\n",
       " '▁200',\n",
       " 'de',\n",
       " '\\n|▁ש',\n",
       " 'קל',\n",
       " 'ם▁ל',\n",
       " 'מה▁',\n",
       " 'קופ',\n",
       " 'ך▁',\n",
       " 'סדר',\n",
       " '▁הת',\n",
       " '\"▁|',\n",
       " 'עד',\n",
       " '--',\n",
       " 'מח',\n",
       " 'נא',\n",
       " 'פח',\n",
       " '.jpg',\n",
       " 'math',\n",
       " 'קוב',\n",
       " '▁=',\n",
       " 'שת',\n",
       " 'ה\"',\n",
       " 'גרסאו',\n",
       " '▁בו',\n",
       " 'ריי',\n",
       " 'מדי',\n",
       " 'טור',\n",
       " 'גור',\n",
       " 'תפקיד',\n",
       " 'בוד',\n",
       " 'נות',\n",
       " 'ליי',\n",
       " 'זיק',\n",
       " 'ים▁והי',\n",
       " 'אישור',\n",
       " 'תרים',\n",
       " 'רות▁',\n",
       " 'הר',\n",
       " 'על▁',\n",
       " 'מס',\n",
       " 'חדש',\n",
       " 'ing',\n",
       " 'ol',\n",
       " 'מור',\n",
       " 'ייר',\n",
       " 'נה▁ב',\n",
       " 'yle',\n",
       " '▁נו',\n",
       " 'דגל',\n",
       " 'טל',\n",
       " 'תיאור',\n",
       " 'אישורים▁והי',\n",
       " 'אישורים▁והיתרים',\n",
       " 'math>',\n",
       " 'פרו',\n",
       " 'גרסאות▁אחר',\n",
       " 'נת▁',\n",
       " '▁כל',\n",
       " '.com',\n",
       " '▁•',\n",
       " 'style',\n",
       " 'גרסאות▁אחרות=',\n",
       " 'שחק',\n",
       " 'רוב',\n",
       " '|\\n|',\n",
       " 'ת▁של▁',\n",
       " 'ign',\n",
       " 'נר',\n",
       " 'נות▁',\n",
       " 'חרי',\n",
       " '30',\n",
       " 'פרס',\n",
       " 'to',\n",
       " 'ם▁א',\n",
       " 'חב',\n",
       " 'style=\"',\n",
       " ',▁ל',\n",
       " 'כול',\n",
       " 'חוז',\n",
       " 'ריק',\n",
       " '▁עם▁',\n",
       " 'ik',\n",
       " 'שפת▁ה',\n",
       " 'ראש',\n",
       " 'ה▁י',\n",
       " '\\t\\t',\n",
       " 'גרסאות▁אחרות=\\n}}',\n",
       " '|כ',\n",
       " '▁שו',\n",
       " '▁בר',\n",
       " 'מת▁ה',\n",
       " '▁מא',\n",
       " 'col',\n",
       " '▁{{',\n",
       " '}}}}',\n",
       " '▁אור',\n",
       " 'ליט',\n",
       " 'סיט',\n",
       " '▁צ',\n",
       " 'ab',\n",
       " '▁וב',\n",
       " '44',\n",
       " '▁וי',\n",
       " 'תוכ',\n",
       " 'יות▁',\n",
       " 'שוב',\n",
       " 'ניבר',\n",
       " '{{הערה|{{',\n",
       " 'ן▁(',\n",
       " 'זכ',\n",
       " '▁יו',\n",
       " '199',\n",
       " '25',\n",
       " 'ם▁בשפת▁ה',\n",
       " 'קול',\n",
       " 'ss',\n",
       " 'נה▁ה',\n",
       " 'נח',\n",
       " 'ערכ',\n",
       " 'align',\n",
       " 'ir',\n",
       " 'מה',\n",
       " 'ליג',\n",
       " '100',\n",
       " '▁הע',\n",
       " 'נוע',\n",
       " 'iv',\n",
       " 'ed',\n",
       " '▁על▁ה',\n",
       " 'אלי',\n",
       " 'מוש',\n",
       " 'ניברסיט',\n",
       " ',▁וה',\n",
       " 'קרי',\n",
       " '▁ביו',\n",
       " 'מים',\n",
       " 'Th',\n",
       " 'שא',\n",
       " 'סל',\n",
       " '▁בת',\n",
       " 'ה▁לה',\n",
       " 'ה▁של▁ה',\n",
       " 'ן▁א',\n",
       " 'ביב',\n",
       " 'גיע',\n",
       " '▁S',\n",
       " '7%',\n",
       " 'קב',\n",
       " 'סטי',\n",
       " 'אתר',\n",
       " 'לאו',\n",
       " 'us',\n",
       " 'wid',\n",
       " '>{{',\n",
       " 'נפ',\n",
       " 'width',\n",
       " 'ad',\n",
       " 'אישורים▁והיתרים={{',\n",
       " 'ag',\n",
       " 'כב',\n",
       " 'מקו',\n",
       " 'אשר',\n",
       " 'עבר',\n",
       " '-s',\n",
       " '\\n|▁מ',\n",
       " 'ל▁ב',\n",
       " '.com/',\n",
       " 'נוי',\n",
       " '50',\n",
       " '\\n|תאריך▁יציר',\n",
       " 'סיי',\n",
       " 'דרו',\n",
       " '▁הצ',\n",
       " '▁מכ',\n",
       " 'of▁',\n",
       " '}}\\n|גרסאות▁אחרות=\\n}}',\n",
       " ',▁וב',\n",
       " '{|▁',\n",
       " 'צרפ',\n",
       " 'כדורגל',\n",
       " 'תבנית',\n",
       " '\\n|-\\n|▁',\n",
       " '▁▁▁▁',\n",
       " '▁עד',\n",
       " '11111|11111|',\n",
       " 'חיד',\n",
       " 'ה▁בי',\n",
       " 'חיל',\n",
       " '▁ט',\n",
       " 'ת.',\n",
       " 'לוש',\n",
       " '▁|תיאור',\n",
       " 'בית▁ה',\n",
       " '▁בש',\n",
       " 'נטי',\n",
       " 'זא',\n",
       " 'כנס',\n",
       " '▁חו',\n",
       " 'תבנית:',\n",
       " '197',\n",
       " 'גדול',\n",
       " 'lo',\n",
       " 'ן.',\n",
       " 'ך▁ל',\n",
       " 'ניה',\n",
       " 'ם▁ש',\n",
       " 'אד',\n",
       " 'ן▁כ',\n",
       " 'ew',\n",
       " '▁פר',\n",
       " 'לב',\n",
       " 'eb',\n",
       " 'מוד',\n",
       " 'לוגי',\n",
       " 'em',\n",
       " '▁הר',\n",
       " 'מת',\n",
       " 'גרמני',\n",
       " '90',\n",
       " 'לאחר',\n",
       " 'D7%',\n",
       " 'ן,▁',\n",
       " 'משחק',\n",
       " 'ra',\n",
       " 'no',\n",
       " '▁הפ',\n",
       " 'ה▁של',\n",
       " 'דגל|',\n",
       " '000',\n",
       " 'טיפ',\n",
       " '▁בע',\n",
       " '26',\n",
       " '▁רו',\n",
       " 'קובץ',\n",
       " 'הוד',\n",
       " 'הב',\n",
       " 'ים▁כ',\n",
       " 'מת▁',\n",
       " ').',\n",
       " 'אירו',\n",
       " 'נה▁ל',\n",
       " '▁סי',\n",
       " '▁M',\n",
       " '▁הראשו',\n",
       " '▁וא',\n",
       " 'ם▁=▁',\n",
       " '▁מטר',\n",
       " '40',\n",
       " 'פולי',\n",
       " 'חס',\n",
       " 'קרב',\n",
       " 'ה▁המ',\n",
       " '80',\n",
       " '▁|תיאור=',\n",
       " '▁קי',\n",
       " '%D7%',\n",
       " 'im',\n",
       " 'דוע',\n",
       " '<re',\n",
       " 'נגלי',\n",
       " 'ורו',\n",
       " 'ame',\n",
       " 'צות▁הברית',\n",
       " '▁קו',\n",
       " 'ם▁כ',\n",
       " '▁A',\n",
       " 'ניס',\n",
       " 'נוסף',\n",
       " 'der',\n",
       " 'ה=\\n|',\n",
       " '-20',\n",
       " 'סמ',\n",
       " 'מוע',\n",
       " '▁על',\n",
       " 'תמש',\n",
       " '▁שב',\n",
       " 'שט',\n",
       " '▁היא',\n",
       " '27',\n",
       " 'חרו',\n",
       " 'wik',\n",
       " 'מבר▁20',\n",
       " 'פעי',\n",
       " 'הודי',\n",
       " '▁C',\n",
       " 'נת',\n",
       " 'ות.',\n",
       " '\\n|▁ה',\n",
       " '{{הערה|שם=',\n",
       " 'תקופ',\n",
       " 'נית▁',\n",
       " 'אלבו',\n",
       " 'נוב',\n",
       " \"ז'\",\n",
       " 'פא',\n",
       " 'נסי',\n",
       " 'זמ',\n",
       " '1.',\n",
       " 'לאומי',\n",
       " 'כלו',\n",
       " '28',\n",
       " 'om',\n",
       " 'ת▁של▁ה',\n",
       " '-0',\n",
       " 'נטר',\n",
       " 'עש',\n",
       " 'קובץ:',\n",
       " '\\n|}',\n",
       " 'ים,▁',\n",
       " 'סטר',\n",
       " 'all',\n",
       " 'פיע',\n",
       " 'ה▁על▁',\n",
       " 'lu',\n",
       " '▁מת',\n",
       " '▁3',\n",
       " 'יוצר=',\n",
       " '▁ב-',\n",
       " 'http://www.',\n",
       " 'חיי',\n",
       " 'ארץ',\n",
       " ')▁{{ש}}',\n",
       " 'שיא',\n",
       " 'סוף',\n",
       " '▁הק',\n",
       " 'טרו',\n",
       " 'tr',\n",
       " 'רק',\n",
       " 'כס',\n",
       " \"▁ג'\",\n",
       " 'ate',\n",
       " '29',\n",
       " 'ת▁(',\n",
       " 'כתב',\n",
       " 'as',\n",
       " '\"מ',\n",
       " '60',\n",
       " 'רש',\n",
       " 'ריו',\n",
       " 'ויזי',\n",
       " 'נוספ',\n",
       " 'ייס',\n",
       " 'מצא',\n",
       " 'הת',\n",
       " 'ה▁נ',\n",
       " 'קישור',\n",
       " '▁|▁ש',\n",
       " '▁במהל',\n",
       " 'גוסט',\n",
       " 'ter',\n",
       " 'שני',\n",
       " 'ים▁(',\n",
       " 'width=\"',\n",
       " '▁ביותר',\n",
       " 'טוב',\n",
       " 'name',\n",
       " 'רבי',\n",
       " '198',\n",
       " 'The▁',\n",
       " '▁שני',\n",
       " 'ורט',\n",
       " 'צבע',\n",
       " '▁ני',\n",
       " 'אב',\n",
       " 'גוב',\n",
       " 'ת▁המ',\n",
       " 'נות▁ה',\n",
       " 'משך',\n",
       " 'קרו',\n",
       " 's/',\n",
       " 'enter',\n",
       " 'דבר',\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_vocab = {v: k for k,v in new_tokenizer_30k.vocab.items()}\n",
    "list(dict(sorted(rev_vocab.items(), key=lambda item: item[0], reverse=False)).values())[11_000:13_000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11183"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tokenizer_30k.vocab['▁ה']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "my_counter = Counter()\n",
    "\n",
    "vocab = dict(new_tokenizer_30k.vocab)\n",
    "\n",
    "for record in all_wiki_records:\n",
    "    tokenized = new_tokenizer_30k.tokenize(record)\n",
    "    my_counter.update([t for t in tokenized if vocab[t] > 11182])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tkn</th>\n",
       "      <th>cnt</th>\n",
       "      <th>tkn_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4425</th>\n",
       "      <td>&lt;/</td>\n",
       "      <td>3613</td>\n",
       "      <td>335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4430</th>\n",
       "      <td>--</td>\n",
       "      <td>3609</td>\n",
       "      <td>458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5380</th>\n",
       "      <td>גרסאו</td>\n",
       "      <td>2983</td>\n",
       "      <td>468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6131</th>\n",
       "      <td>ים▁והי</td>\n",
       "      <td>2619</td>\n",
       "      <td>479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7069</th>\n",
       "      <td>w.</td>\n",
       "      <td>2252</td>\n",
       "      <td>358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7076</th>\n",
       "      <td>קבו</td>\n",
       "      <td>2250</td>\n",
       "      <td>272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7089</th>\n",
       "      <td>אישור</td>\n",
       "      <td>2246</td>\n",
       "      <td>480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7382</th>\n",
       "      <td>צות▁הברי</td>\n",
       "      <td>2159</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7613</th>\n",
       "      <td>תרים</td>\n",
       "      <td>2091</td>\n",
       "      <td>481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8463</th>\n",
       "      <td>//</td>\n",
       "      <td>1866</td>\n",
       "      <td>241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8698</th>\n",
       "      <td>www.</td>\n",
       "      <td>1812</td>\n",
       "      <td>377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9744</th>\n",
       "      <td>math</td>\n",
       "      <td>1607</td>\n",
       "      <td>463</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10108</th>\n",
       "      <td>1111</td>\n",
       "      <td>1542</td>\n",
       "      <td>309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10301</th>\n",
       "      <td>.j</td>\n",
       "      <td>1512</td>\n",
       "      <td>443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10366</th>\n",
       "      <td>מקור=</td>\n",
       "      <td>1500</td>\n",
       "      <td>438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11126</th>\n",
       "      <td>ww</td>\n",
       "      <td>1391</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11366</th>\n",
       "      <td>תאריך▁יציר</td>\n",
       "      <td>1360</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11543</th>\n",
       "      <td>{{ש</td>\n",
       "      <td>1336</td>\n",
       "      <td>163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12729</th>\n",
       "      <td>.co</td>\n",
       "      <td>1202</td>\n",
       "      <td>388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12898</th>\n",
       "      <td>pg</td>\n",
       "      <td>1186</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14438</th>\n",
       "      <td>tp</td>\n",
       "      <td>1054</td>\n",
       "      <td>243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16135</th>\n",
       "      <td>yle</td>\n",
       "      <td>937</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17614</th>\n",
       "      <td>אישורים▁והיתרים</td>\n",
       "      <td>699</td>\n",
       "      <td>498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17938</th>\n",
       "      <td>תפקי</td>\n",
       "      <td>416</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17956</th>\n",
       "      <td>11111|</td>\n",
       "      <td>403</td>\n",
       "      <td>392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18316</th>\n",
       "      <td>http</td>\n",
       "      <td>148</td>\n",
       "      <td>247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18436</th>\n",
       "      <td>https</td>\n",
       "      <td>83</td>\n",
       "      <td>426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18466</th>\n",
       "      <td>://</td>\n",
       "      <td>66</td>\n",
       "      <td>248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18494</th>\n",
       "      <td>אישורים▁והי</td>\n",
       "      <td>55</td>\n",
       "      <td>497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18619</th>\n",
       "      <td>://www.</td>\n",
       "      <td>14</td>\n",
       "      <td>411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   tkn   cnt  tkn_id\n",
       "4425                </  3613     335\n",
       "4430                --  3609     458\n",
       "5380             גרסאו  2983     468\n",
       "6131            ים▁והי  2619     479\n",
       "7069                w.  2252     358\n",
       "7076               קבו  2250     272\n",
       "7089             אישור  2246     480\n",
       "7382          צות▁הברי  2159     387\n",
       "7613              תרים  2091     481\n",
       "8463                //  1866     241\n",
       "8698              www.  1812     377\n",
       "9744              math  1607     463\n",
       "10108             1111  1542     309\n",
       "10301               .j  1512     443\n",
       "10366            מקור=  1500     438\n",
       "11126               ww  1391     205\n",
       "11366       תאריך▁יציר  1360     383\n",
       "11543              {{ש  1336     163\n",
       "12729              .co  1202     388\n",
       "12898               pg  1186     433\n",
       "14438               tp  1054     243\n",
       "16135              yle   937     492\n",
       "17614  אישורים▁והיתרים   699     498\n",
       "17938             תפקי   416     326\n",
       "17956           11111|   403     392\n",
       "18316             http   148     247\n",
       "18436            https    83     426\n",
       "18466              ://    66     248\n",
       "18494      אישורים▁והי    55     497\n",
       "18619          ://www.    14     411"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "as_df = pd.DataFrame(my_counter.most_common(30_000), columns=['tkn','cnt'])\n",
    "# as_df['cnt'].hist()\n",
    "\n",
    "as_df['tkn_id'] = as_df['tkn'].apply(lambda t: vocab[t] - 11182)\n",
    "\n",
    "# as_df[(as_df['cnt'] < 1_000) & (as_df['cnt'] > 800)]['tkn']\n",
    "\n",
    "\n",
    "\n",
    "as_df[(as_df['cnt'] < 4000) & (as_df['tkn_id'] < 500)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocab_pos</th>\n",
       "      <th>less_frequent_rank</th>\n",
       "      <th>tkn</th>\n",
       "      <th>cnt</th>\n",
       "      <th>unique_but_start_dict_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>248</td>\n",
       "      <td>254</td>\n",
       "      <td>://</td>\n",
       "      <td>66</td>\n",
       "      <td>314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>404</th>\n",
       "      <td>247</td>\n",
       "      <td>404</td>\n",
       "      <td>http</td>\n",
       "      <td>148</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>411</td>\n",
       "      <td>101</td>\n",
       "      <td>://www.</td>\n",
       "      <td>14</td>\n",
       "      <td>425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>426</td>\n",
       "      <td>284</td>\n",
       "      <td>https</td>\n",
       "      <td>83</td>\n",
       "      <td>509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>497</td>\n",
       "      <td>226</td>\n",
       "      <td>אישורים▁והי</td>\n",
       "      <td>55</td>\n",
       "      <td>552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18716</th>\n",
       "      <td>42</td>\n",
       "      <td>18716</td>\n",
       "      <td>ה▁ב</td>\n",
       "      <td>238732</td>\n",
       "      <td>238774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18717</th>\n",
       "      <td>28</td>\n",
       "      <td>18717</td>\n",
       "      <td>,▁</td>\n",
       "      <td>245319</td>\n",
       "      <td>245347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18718</th>\n",
       "      <td>11</td>\n",
       "      <td>18718</td>\n",
       "      <td>▁ל</td>\n",
       "      <td>270323</td>\n",
       "      <td>270334</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18719</th>\n",
       "      <td>1</td>\n",
       "      <td>18719</td>\n",
       "      <td>▁ה</td>\n",
       "      <td>285420</td>\n",
       "      <td>285421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18720</th>\n",
       "      <td>2</td>\n",
       "      <td>18720</td>\n",
       "      <td>▁ב</td>\n",
       "      <td>321352</td>\n",
       "      <td>321354</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18721 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       vocab_pos  less_frequent_rank          tkn     cnt  \\\n",
       "254          248                 254          ://      66   \n",
       "404          247                 404         http     148   \n",
       "101          411                 101      ://www.      14   \n",
       "284          426                 284        https      83   \n",
       "226          497                 226  אישורים▁והי      55   \n",
       "...          ...                 ...          ...     ...   \n",
       "18716         42               18716          ה▁ב  238732   \n",
       "18717         28               18717           ,▁  245319   \n",
       "18718         11               18718           ▁ל  270323   \n",
       "18719          1               18719           ▁ה  285420   \n",
       "18720          2               18720           ▁ב  321352   \n",
       "\n",
       "       unique_but_start_dict_rank  \n",
       "254                           314  \n",
       "404                           395  \n",
       "101                           425  \n",
       "284                           509  \n",
       "226                           552  \n",
       "...                           ...  \n",
       "18716                      238774  \n",
       "18717                      245347  \n",
       "18718                      270334  \n",
       "18719                      285421  \n",
       "18720                      321354  \n",
       "\n",
       "[18721 rows x 5 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with_idx = [(idx, item[0], item[1]) for idx,item in enumerate(reversed(my_counter.most_common(30_000)))]\n",
    "\n",
    "with_vocab_pos = [(vocab[item[1]] - 11182, item[0], item[1], item[2]) for item in with_idx]\n",
    "\n",
    "cnt_df = pd.DataFrame(with_vocab_pos, columns=['vocab_pos', 'less_frequent_rank', 'tkn', 'cnt'])\n",
    "cnt_df['unique_but_start_dict_rank'] = cnt_df.apply(lambda x: x['vocab_pos'] + x['cnt'], axis=1)\n",
    "\n",
    "\n",
    "cnt_df.sort_values('unique_but_start_dict_rank')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/30k_tokenizer/tokenizer_config.json',\n",
       " 'data/30k_tokenizer/special_tokens_map.json',\n",
       " 'data/30k_tokenizer/tokenizer.model',\n",
       " 'data/30k_tokenizer/added_tokens.json',\n",
       " 'data/30k_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(cnt_df, open('data/cnt_df.pkl', 'wb'))\n",
    "new_tokenizer_30k.save_pretrained('data/30k_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "cnt_df = pickle.load(open('data/cnt_df.pkl', 'rb'))\n",
    "new_tokenizer_30k = AutoTokenizer.from_pretrained('data/30k_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_path' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 14\u001b[0m\n\u001b[1;32m      8\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# model_path = 'meta-llama/Llama-3.1-8B'\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# model_name = 'llama_3_1'\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[43mmodel_path\u001b[49m, token\u001b[38;5;241m=\u001b[39mtoken, torch_dtype\u001b[38;5;241m=\u001b[39mdtype)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_path, token\u001b[38;5;241m=\u001b[39mtoken)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model_path' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from src.tokens2words.word_retriever import AnalysisWordRetriever\n",
    "from src.tokens2words.utils.enums import MultiTokenKind\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# model_path = 'meta-llama/Llama-3.1-8B'\n",
    "# model_name = 'llama_3_1'\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, token=token, torch_dtype=dtype).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, token=token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_retriever = AnalysisWordRetriever(model, tokenizer, MultiTokenKind.Natural, add_context=True,\n",
    "                                model_name=model_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_cnt = cnt_df[~cnt_df['tkn'].str.contains('▁')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_token_layers(record):\n",
    "    original_tkn = record['tkn']\n",
    "    tokenized_combined_text = tokenizer([original_tkn], return_tensors='pt', truncation=True, max_length=5).to(device)\n",
    "\n",
    "    predicted_layers = []\n",
    "    predicted_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized_combined_text, output_hidden_states=True)\n",
    "\n",
    "    hidden_states = outputs.hidden_states\n",
    "    for layer_idx, hidden_state in enumerate(hidden_states):\n",
    "        postfix_hidden_state = hidden_states[layer_idx][0, -1, :].unsqueeze(0)\n",
    "        retrieved_word_str = word_retriever.retriever.retrieve_word(postfix_hidden_state, layer_idx=layer_idx,\n",
    "                                                            num_tokens_to_generate=tokenized_combined_text['input_ids'].shape[1])\n",
    "        if retrieved_word_str[0].strip() == original_tkn:\n",
    "            predicted_layers.append(layer_idx)\n",
    "        predicted_values.append(original_tkn)\n",
    "    record['predicted_layers'] = predicted_layers\n",
    "    record['predicted_values'] = predicted_values\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_cnt = filtered_cnt.apply(get_token_layers, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "pickle.dump(filtered_cnt, open('data/filtered_cnt.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "from collections import Counter\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from src.tokens2words.word_retriever import AnalysisWordRetriever\n",
    "from src.tokens2words.utils.enums import MultiTokenKind\n",
    "\n",
    "model_path = 'meta-llama/Llama-3.1-8B'\n",
    "model_name = 'llama_3_1'\n",
    "token = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_path, token=token)\n",
    "new_tokenizer_30k = tokenizer.train_new_from_iterator(iter(all_wiki_records), vocab_size=30_000)\n",
    "rev_vocab = {v: k for k,v in new_tokenizer_30k.vocab.items()}\n",
    "\n",
    "\n",
    "my_counter = Counter()\n",
    "\n",
    "vocab = dict(new_tokenizer_30k.vocab)\n",
    "\n",
    "# original_vocab_token_cnt = 11182\n",
    "\n",
    "original_vocab_token_cnt = 0\n",
    "for tkn in list(dict(sorted(rev_vocab.items(), key=lambda item: item[0], reverse=False)).values()):\n",
    "    original_vocab_token_cnt += 1\n",
    "    if len([t for t in tokenizer.encode(tkn) if t != 128000]) > 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('data/llama3_30k_tokenizer/tokenizer_config.json',\n",
       " 'data/llama3_30k_tokenizer/special_tokens_map.json',\n",
       " 'data/llama3_30k_tokenizer/tokenizer.json')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for record in all_wiki_records:\n",
    "    tokenized = new_tokenizer_30k.tokenize(record)\n",
    "    my_counter.update([t for t in tokenized if vocab[t] > original_vocab_token_cnt])\n",
    "\n",
    "with_idx = [(idx, item[0], item[1]) for idx,item in enumerate(reversed(my_counter.most_common(30_000)))]\n",
    "\n",
    "with_vocab_pos = [(vocab[item[1]] - original_vocab_token_cnt, item[0], item[1], item[2]) for item in with_idx]\n",
    "\n",
    "cnt_df = pd.DataFrame(with_vocab_pos, columns=['vocab_pos', 'less_frequent_rank', 'tkn', 'cnt'])\n",
    "cnt_df['unique_but_start_dict_rank'] = cnt_df.apply(lambda x: x['vocab_pos'] + x['cnt'], axis=1)\n",
    "\n",
    "\n",
    "cnt_df.sort_values('unique_but_start_dict_rank')\n",
    "\n",
    "\n",
    "pickle.dump(cnt_df, open('data/llama3_cnt_df.pkl', 'wb'))\n",
    "new_tokenizer_30k.save_pretrained('data/llama3_30k_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_df = pickle.load(open('data/llama3_cnt_df.pkl', 'rb'))\n",
    "new_tokenizer_30k = AutoTokenizer.from_pretrained('data/llama3_30k_tokenizer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnt_df['original_tkn'] = cnt_df['vocab_pos'].apply(lambda vp: rev_vocab[vp + original_vocab_token_cnt])\n",
    "cnt_df['original_tkn'] = cnt_df['tkn']\n",
    "\n",
    "vocab = new_tokenizer_30k.vocab\n",
    "\n",
    "cnt_df['tkn'] = cnt_df['original_tkn'].apply(lambda t: new_tokenizer_30k.decode(vocab[t]))\n",
    "\n",
    "filtered_cnt = cnt_df[~cnt_df['tkn'].str.contains('�')].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc3ba3945904f358a74b1629651a10a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "\n",
    "dtype = torch.bfloat16\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, token=token, torch_dtype=dtype).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, token=token)\n",
    "\n",
    "word_retriever = AnalysisWordRetriever(model, tokenizer, MultiTokenKind.Natural, add_context=True,\n",
    "                                model_name=model_name, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token_layers(record):\n",
    "    original_tkn = record['tkn']\n",
    "    tokenized_combined_text = tokenizer([original_tkn], return_tensors='pt', truncation=True, max_length=5).to(device)\n",
    "\n",
    "    predicted_layers = []\n",
    "    predicted_values = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokenized_combined_text, output_hidden_states=True)\n",
    "\n",
    "    hidden_states = outputs.hidden_states\n",
    "    for layer_idx, hidden_state in enumerate(hidden_states):\n",
    "        postfix_hidden_state = hidden_states[layer_idx][0, -1, :].unsqueeze(0)\n",
    "        retrieved_word_str = word_retriever.retriever.retrieve_word(postfix_hidden_state, layer_idx=layer_idx,\n",
    "                                                            num_tokens_to_generate=tokenized_combined_text['input_ids'].shape[1] + 2)\n",
    "        if retrieved_word_str[0].strip() == original_tkn.strip():\n",
    "            predicted_layers.append(layer_idx)\n",
    "        predicted_values.append(retrieved_word_str[0].strip())\n",
    "    record['predicted_layers'] = predicted_layers\n",
    "    record['predicted_values'] = predicted_values\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_cnt = filtered_cnt.sort_values('less_frequent_rank', ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29411/29411 [29:38:45<00:00,  3.63s/it]   \n"
     ]
    }
   ],
   "source": [
    "all_records = pickle.load(open('data/llama3_filtered_cnt_valid.pkl', 'rb'))\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "for idx, row in tqdm(filtered_cnt.iterrows(), total=len(filtered_cnt)):\n",
    "    cnt +=1\n",
    "    if cnt <= len(all_records):\n",
    "        continue\n",
    "    enriched_row = get_token_layers(row)\n",
    "    all_records.append(enriched_row)\n",
    "    if cnt % 10 == 0:\n",
    "        pickle.dump(all_records, open('data/llama3_filtered_cnt_valid.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_records_df = pd.DataFrame(all_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vocab_pos</th>\n",
       "      <th>less_frequent_rank</th>\n",
       "      <th>tkn</th>\n",
       "      <th>cnt</th>\n",
       "      <th>unique_but_start_dict_rank</th>\n",
       "      <th>original_tkn</th>\n",
       "      <th>predicted_layers</th>\n",
       "      <th>predicted_values</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7030</td>\n",
       "      <td>0</td>\n",
       "      <td>מתקי</td>\n",
       "      <td>3</td>\n",
       "      <td>7033</td>\n",
       "      <td>Ġ×ŀ×ª×§×Ļ</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ מתקי,  מתקי,  מתקי,  מתקי,  מתקי,  מתקי,  מת...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13775</td>\n",
       "      <td>1</td>\n",
       "      <td>אפגנ</td>\n",
       "      <td>3</td>\n",
       "      <td>13778</td>\n",
       "      <td>Ġ×Ĳ×¤×Ĵ×ł</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ אפגנ,  אפגנ,  אפגנ,  אפגנ,  אפגנ,  אפגנ,  אפ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7818</td>\n",
       "      <td>3</td>\n",
       "      <td>שהתקי</td>\n",
       "      <td>6</td>\n",
       "      <td>7824</td>\n",
       "      <td>Ġ×©×Ķ×ª×§×Ļ</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ שהתקי,  שהתקי,  שהתקי,  שהתקי,  שהתקי,  שהתק...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16316</td>\n",
       "      <td>5</td>\n",
       "      <td>אימצ</td>\n",
       "      <td>6</td>\n",
       "      <td>16322</td>\n",
       "      <td>Ġ×Ĳ×Ļ×ŀ×¦</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ אימצ,  אימצ,  אימצ,  אימצ,  אימצ,  אימצ,  אי...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13708</td>\n",
       "      <td>6</td>\n",
       "      <td>רפוא</td>\n",
       "      <td>9</td>\n",
       "      <td>13717</td>\n",
       "      <td>Ġ×¨×¤×ķ×Ĳ</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ רפוא,  רפוא,  רפוא,  רפוא,  רפוא,  רפוא,  רפ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29406</th>\n",
       "      <td>131</td>\n",
       "      <td>29511</td>\n",
       "      <td>=</td>\n",
       "      <td>1978188</td>\n",
       "      <td>1978319</td>\n",
       "      <td>Ġ=</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ =,  =,  =,  =,  =,  =,  =,  =,  =,  =,  =,  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29407</th>\n",
       "      <td>132</td>\n",
       "      <td>29512</td>\n",
       "      <td>(</td>\n",
       "      <td>2141322</td>\n",
       "      <td>2141454</td>\n",
       "      <td>Ġ(</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ (,  (,  (,  (,  (,  (,  (,  (,  (,  (,  (,  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29408</th>\n",
       "      <td>109</td>\n",
       "      <td>29513</td>\n",
       "      <td>של</td>\n",
       "      <td>3153417</td>\n",
       "      <td>3153526</td>\n",
       "      <td>Ġ×©×ľ</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ של,  של,  של,  של,  של,  של,  של,  של,  של, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29409</th>\n",
       "      <td>10</td>\n",
       "      <td>29514</td>\n",
       "      <td>\\n</td>\n",
       "      <td>4449807</td>\n",
       "      <td>4449817</td>\n",
       "      <td>Ċ</td>\n",
       "      <td>[]</td>\n",
       "      <td>[\\n, \\n, \\n, \\n, \\n, \\n, \\n, \\n, \\n, \\n, \\n, \\...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29410</th>\n",
       "      <td>32</td>\n",
       "      <td>29515</td>\n",
       "      <td></td>\n",
       "      <td>6462111</td>\n",
       "      <td>6462143</td>\n",
       "      <td>Ġ</td>\n",
       "      <td>[]</td>\n",
       "      <td>[ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29411 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       vocab_pos  less_frequent_rank     tkn      cnt  \\\n",
       "0           7030                   0    מתקי        3   \n",
       "1          13775                   1    אפגנ        3   \n",
       "2           7818                   3   שהתקי        6   \n",
       "3          16316                   5    אימצ        6   \n",
       "4          13708                   6    רפוא        9   \n",
       "...          ...                 ...     ...      ...   \n",
       "29406        131               29511       =  1978188   \n",
       "29407        132               29512       (  2141322   \n",
       "29408        109               29513      של  3153417   \n",
       "29409         10               29514      \\n  4449807   \n",
       "29410         32               29515          6462111   \n",
       "\n",
       "       unique_but_start_dict_rank original_tkn predicted_layers  \\\n",
       "0                            7033    Ġ×ŀ×ª×§×Ļ               []   \n",
       "1                           13778    Ġ×Ĳ×¤×Ĵ×ł               []   \n",
       "2                            7824  Ġ×©×Ķ×ª×§×Ļ               []   \n",
       "3                           16322    Ġ×Ĳ×Ļ×ŀ×¦               []   \n",
       "4                           13717    Ġ×¨×¤×ķ×Ĳ               []   \n",
       "...                           ...          ...              ...   \n",
       "29406                     1978319           Ġ=               []   \n",
       "29407                     2141454           Ġ(               []   \n",
       "29408                     3153526        Ġ×©×ľ               []   \n",
       "29409                     4449817            Ċ               []   \n",
       "29410                     6462143            Ġ               []   \n",
       "\n",
       "                                        predicted_values  \n",
       "0      [ מתקי,  מתקי,  מתקי,  מתקי,  מתקי,  מתקי,  מת...  \n",
       "1      [ אפגנ,  אפגנ,  אפגנ,  אפגנ,  אפגנ,  אפגנ,  אפ...  \n",
       "2      [ שהתקי,  שהתקי,  שהתקי,  שהתקי,  שהתקי,  שהתק...  \n",
       "3      [ אימצ,  אימצ,  אימצ,  אימצ,  אימצ,  אימצ,  אי...  \n",
       "4      [ רפוא,  רפוא,  רפוא,  רפוא,  רפוא,  רפוא,  רפ...  \n",
       "...                                                  ...  \n",
       "29406  [ =,  =,  =,  =,  =,  =,  =,  =,  =,  =,  =,  ...  \n",
       "29407  [ (,  (,  (,  (,  (,  (,  (,  (,  (,  (,  (,  ...  \n",
       "29408  [ של,  של,  של,  של,  של,  של,  של,  של,  של, ...  \n",
       "29409  [\\n, \\n, \\n, \\n, \\n, \\n, \\n, \\n, \\n, \\n, \\n, \\...  \n",
       "29410  [ ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  ,  , ...  \n",
       "\n",
       "[29411 rows x 8 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_records_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'מתקי'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rev_vocab = {v:k for k,v in new_tokenizer_30k.vocab.items()}\n",
    "new_tokenizer_30k.decode(new_tokenizer_30k.vocab['Ġ×ŀ×ª×§×Ļ']).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי',\n",
       " ' מתקי']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_records_df.iloc[0]['predicted_values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['headers',\n",
       " 'iful',\n",
       " 'BSC',\n",
       " '.gs',\n",
       " 'layout',\n",
       " 'suggest',\n",
       " 'collapse',\n",
       " 'BAD',\n",
       " 'TRL',\n",
       " 'operator',\n",
       " 'required',\n",
       " 'EDA',\n",
       " 'ublisher',\n",
       " 'icros',\n",
       " 'spacing',\n",
       " 'attern',\n",
       " 'Db',\n",
       " 'uggest',\n",
       " 'ilon',\n",
       " 'adding',\n",
       " 'iforn',\n",
       " 'quired',\n",
       " '.blog',\n",
       " 'ACK',\n",
       " '_-',\n",
       " '.tele',\n",
       " '.ny',\n",
       " 'spot',\n",
       " 'sq',\n",
       " 'dead',\n",
       " 'ա',\n",
       " '-work',\n",
       " 'Marker',\n",
       " 'true',\n",
       " '/upload',\n",
       " 'dep',\n",
       " 'alem',\n",
       " 'lymp',\n",
       " 'ware',\n",
       " 'otify',\n",
       " 'fault',\n",
       " 'acebook',\n",
       " 'peg',\n",
       " 'latin',\n",
       " 'ecause',\n",
       " 'ASA',\n",
       " 'agram',\n",
       " 'hebrew',\n",
       " 'toc',\n",
       " 'cele',\n",
       " 'apps',\n",
       " 'iness',\n",
       " 'reference',\n",
       " 'cto',\n",
       " '.app',\n",
       " '.city',\n",
       " 'ample',\n",
       " 'outube',\n",
       " 'zet',\n",
       " 'heel',\n",
       " 'xford',\n",
       " 'Off',\n",
       " 'andom',\n",
       " 'depend',\n",
       " 'eca',\n",
       " 'return',\n",
       " 'ipes',\n",
       " 'HF',\n",
       " 'rys',\n",
       " 'ilit',\n",
       " 'ampion',\n",
       " 'bum',\n",
       " 'irthday',\n",
       " 'region',\n",
       " 'ictures',\n",
       " 'crip',\n",
       " 'ictionary',\n",
       " 'ircle',\n",
       " 'quote',\n",
       " 'fn',\n",
       " 'BN',\n",
       " '-we',\n",
       " '/play',\n",
       " 'volume',\n",
       " 'stitute',\n",
       " 'igma',\n",
       " 'people',\n",
       " 'itation',\n",
       " 'עזר',\n",
       " 'זרע',\n",
       " 'ival',\n",
       " 'archives',\n",
       " '.im',\n",
       " 'unction',\n",
       " 'ael',\n",
       " 'onom',\n",
       " 'ground',\n",
       " 'include',\n",
       " 'gypt',\n",
       " 'board',\n",
       " 'Gov',\n",
       " 'apers',\n",
       " 'arge',\n",
       " 'eps',\n",
       " 'location',\n",
       " 'ega',\n",
       " 'cogs',\n",
       " 'assy',\n",
       " '.dis',\n",
       " 'ervice',\n",
       " 'Arch',\n",
       " 'option',\n",
       " 'Wiki',\n",
       " 'cis',\n",
       " 'reate',\n",
       " ':s',\n",
       " 'VD',\n",
       " 'ema',\n",
       " 'rive',\n",
       " 'nal',\n",
       " 'BE',\n",
       " '937',\n",
       " 'wig',\n",
       " 'CP',\n",
       " 'etting',\n",
       " 'aves',\n",
       " 'об',\n",
       " '/The',\n",
       " 'ulu',\n",
       " '038',\n",
       " 'גרש',\n",
       " 'abel',\n",
       " '945',\n",
       " '798',\n",
       " '938',\n",
       " 'ullet',\n",
       " 'population',\n",
       " 'irk',\n",
       " 'uropean',\n",
       " '879',\n",
       " 'Ah',\n",
       " 'phys',\n",
       " '-last',\n",
       " 'ֿ',\n",
       " 'bout',\n",
       " 'Mod',\n",
       " 'udi',\n",
       " '?s',\n",
       " '959',\n",
       " 'cor',\n",
       " 'ут',\n",
       " '922',\n",
       " '838',\n",
       " '034',\n",
       " '.news',\n",
       " 'emi',\n",
       " 'uts',\n",
       " 'UG',\n",
       " 'adow',\n",
       " 'ustral',\n",
       " 'щ',\n",
       " 'rah',\n",
       " 'American',\n",
       " '-review',\n",
       " 'עבד',\n",
       " 'asp',\n",
       " '.all',\n",
       " 'elm',\n",
       " '836',\n",
       " 'artin',\n",
       " 'rev',\n",
       " 'Wik',\n",
       " 'iped',\n",
       " '.by',\n",
       " '977',\n",
       " '045',\n",
       " 'abe',\n",
       " 'ols',\n",
       " 'PDF',\n",
       " 'equ',\n",
       " 'ַן',\n",
       " 'Ew',\n",
       " 'ublish',\n",
       " 'imal',\n",
       " 'front',\n",
       " '896',\n",
       " '849',\n",
       " 'pret',\n",
       " 'cup',\n",
       " 'اد',\n",
       " 'iamond',\n",
       " '082',\n",
       " 'agn',\n",
       " '.H',\n",
       " 'ecret',\n",
       " '936',\n",
       " '/al',\n",
       " '987',\n",
       " '954',\n",
       " 'andy',\n",
       " '779',\n",
       " 'esis',\n",
       " 'rog',\n",
       " 'archiv',\n",
       " 'Item',\n",
       " '-dis',\n",
       " '=yes',\n",
       " '898',\n",
       " 'don',\n",
       " '962',\n",
       " 'vertical',\n",
       " 'ells',\n",
       " 'enberg',\n",
       " 'VA',\n",
       " '-ed',\n",
       " 'ENT',\n",
       " '024',\n",
       " 'fm',\n",
       " 'דוב',\n",
       " '839',\n",
       " 'anth',\n",
       " '994',\n",
       " '829',\n",
       " 'arctica',\n",
       " 'audi',\n",
       " '843',\n",
       " 'loss',\n",
       " '.dk',\n",
       " 'ande',\n",
       " '892',\n",
       " 'amps',\n",
       " '_x',\n",
       " 'een',\n",
       " '782',\n",
       " 'OG',\n",
       " 'SE',\n",
       " 'phen',\n",
       " '968',\n",
       " '047',\n",
       " '995',\n",
       " 'lotte',\n",
       " 'Delta',\n",
       " '793',\n",
       " 'vas',\n",
       " '099',\n",
       " 'works',\n",
       " 'их',\n",
       " '797',\n",
       " 'Omega',\n",
       " 'overn',\n",
       " '739',\n",
       " 'Archive',\n",
       " '884',\n",
       " '877',\n",
       " '.is',\n",
       " 'fd',\n",
       " '915',\n",
       " '932',\n",
       " 'Б',\n",
       " 'dom',\n",
       " '955',\n",
       " '035',\n",
       " '799',\n",
       " '927',\n",
       " 'atz',\n",
       " '871',\n",
       " '.eu',\n",
       " 'flo',\n",
       " '873',\n",
       " '784',\n",
       " 'erve',\n",
       " 'aim',\n",
       " 'EED',\n",
       " 'VO',\n",
       " 'hor',\n",
       " '963',\n",
       " '.go',\n",
       " '_news',\n",
       " 'ovo',\n",
       " 'display',\n",
       " ')(',\n",
       " '874',\n",
       " '-str',\n",
       " '942',\n",
       " 'utter',\n",
       " '°C',\n",
       " 'dr',\n",
       " 'hp',\n",
       " '934',\n",
       " 'blockquote',\n",
       " 'andra',\n",
       " '929',\n",
       " '762',\n",
       " ':g',\n",
       " 'uri',\n",
       " 'ير',\n",
       " 'bes',\n",
       " 'olymp',\n",
       " '/photo',\n",
       " 'ources',\n",
       " '895',\n",
       " 'זאת',\n",
       " 'cu',\n",
       " 'What',\n",
       " '985',\n",
       " '687',\n",
       " 'ō',\n",
       " 'GM',\n",
       " '923',\n",
       " '756',\n",
       " 'razil',\n",
       " 'sta',\n",
       " '991',\n",
       " 'ION',\n",
       " '_r',\n",
       " '969',\n",
       " '893',\n",
       " 'ledge',\n",
       " 'options',\n",
       " 'NE',\n",
       " '842',\n",
       " 'ube',\n",
       " 'zog',\n",
       " 'cho',\n",
       " 'ired',\n",
       " 'لي',\n",
       " '/chart',\n",
       " 'iu',\n",
       " 'ning',\n",
       " '&M',\n",
       " 'frame',\n",
       " '948',\n",
       " 'aux',\n",
       " '-series',\n",
       " 'AK',\n",
       " 'ME',\n",
       " 'Type',\n",
       " 'appen',\n",
       " '881',\n",
       " '869',\n",
       " 'SP',\n",
       " 'volution',\n",
       " 'ification',\n",
       " '.tr',\n",
       " '822',\n",
       " '676',\n",
       " 'unes',\n",
       " 'erk',\n",
       " '914',\n",
       " 'more',\n",
       " 'asia',\n",
       " 'vy',\n",
       " 'so',\n",
       " 'otor',\n",
       " 'atas',\n",
       " 'helm',\n",
       " 'atre',\n",
       " '.P',\n",
       " 'opp',\n",
       " '692',\n",
       " '961',\n",
       " '-ar',\n",
       " 'ervices',\n",
       " 'ding',\n",
       " '-rel',\n",
       " 'urrent',\n",
       " 'Coord',\n",
       " 'tro',\n",
       " 'UP',\n",
       " '837',\n",
       " 'cription',\n",
       " 'ín',\n",
       " 'agy',\n",
       " 'active',\n",
       " '988',\n",
       " '688',\n",
       " 'פרח',\n",
       " 'Me',\n",
       " '.id',\n",
       " 'rin',\n",
       " '=true',\n",
       " '971',\n",
       " 'lit',\n",
       " '854',\n",
       " 'oftware',\n",
       " '785',\n",
       " 'asket',\n",
       " 'sf',\n",
       " 'refer',\n",
       " '686',\n",
       " 'Br',\n",
       " '/people',\n",
       " '925',\n",
       " '861',\n",
       " '=a',\n",
       " 'John',\n",
       " 'athan',\n",
       " ',b',\n",
       " '841',\n",
       " ':b',\n",
       " '862',\n",
       " 'oor',\n",
       " '882',\n",
       " '738',\n",
       " 'illage',\n",
       " '759',\n",
       " '983',\n",
       " '848',\n",
       " 'tele',\n",
       " 'entral',\n",
       " 'amin',\n",
       " 'Car',\n",
       " 'צמ',\n",
       " '796',\n",
       " 'HE',\n",
       " 'engine',\n",
       " 'alen',\n",
       " 'citation',\n",
       " 'well',\n",
       " '718',\n",
       " '732',\n",
       " '817',\n",
       " 'Ele',\n",
       " '772',\n",
       " 'ère',\n",
       " 'itions',\n",
       " 'AQ',\n",
       " 'NS',\n",
       " 'ynam',\n",
       " 'icro',\n",
       " 'eq',\n",
       " '.nz',\n",
       " 'quire',\n",
       " 'ived',\n",
       " '083',\n",
       " 'ocaust',\n",
       " 'rb',\n",
       " '-art',\n",
       " '072',\n",
       " 'late',\n",
       " '.ed',\n",
       " '868',\n",
       " 'uke',\n",
       " 'ак',\n",
       " '679',\n",
       " 'esch',\n",
       " '819',\n",
       " 'rait',\n",
       " 'this',\n",
       " 'agement',\n",
       " 'artment',\n",
       " '049',\n",
       " 'uide',\n",
       " '867',\n",
       " 'uba',\n",
       " 'jan',\n",
       " 'elo',\n",
       " 'ieg',\n",
       " '825',\n",
       " '958',\n",
       " '697',\n",
       " 'iran',\n",
       " 'amar',\n",
       " '864',\n",
       " 'show',\n",
       " '951',\n",
       " 'hu',\n",
       " '\\\\cdot',\n",
       " '886',\n",
       " '851',\n",
       " 'arker',\n",
       " 'ige',\n",
       " '921',\n",
       " '-or',\n",
       " '993',\n",
       " '713',\n",
       " '018',\n",
       " '748',\n",
       " 'lain',\n",
       " '674',\n",
       " '037',\n",
       " '823',\n",
       " 'alse',\n",
       " '827',\n",
       " 'orks',\n",
       " 'eden',\n",
       " 'rc',\n",
       " 'href',\n",
       " '659',\n",
       " '055',\n",
       " 'round',\n",
       " 'AV',\n",
       " '946',\n",
       " '821',\n",
       " 'rug',\n",
       " 'admin',\n",
       " '878',\n",
       " '872',\n",
       " 'blue',\n",
       " '039',\n",
       " 'avel',\n",
       " '781',\n",
       " '826',\n",
       " 'LE',\n",
       " '831',\n",
       " 'RNA',\n",
       " '813',\n",
       " '956',\n",
       " 'Web',\n",
       " '.u',\n",
       " '792',\n",
       " 'UT',\n",
       " 'ollywood',\n",
       " '916',\n",
       " '729',\n",
       " 'omer',\n",
       " '899',\n",
       " '919',\n",
       " '795',\n",
       " '042',\n",
       " 'cap',\n",
       " 'quest',\n",
       " 'wg',\n",
       " 'stor',\n",
       " '648',\n",
       " '744',\n",
       " '939',\n",
       " 'rome',\n",
       " 'Official',\n",
       " ':black',\n",
       " 'Pe',\n",
       " '885',\n",
       " '930',\n",
       " '941',\n",
       " '>O',\n",
       " '931',\n",
       " '-car',\n",
       " 'otten',\n",
       " 'cord',\n",
       " 'rep',\n",
       " 'lines',\n",
       " 'imag',\n",
       " 'lov',\n",
       " 'history',\n",
       " 'east',\n",
       " '773',\n",
       " 'אלף',\n",
       " 'ás',\n",
       " '647',\n",
       " '695',\n",
       " '846',\n",
       " '786',\n",
       " 'оп',\n",
       " '-ac',\n",
       " 'estival',\n",
       " '678',\n",
       " '677',\n",
       " 'lick',\n",
       " 'duc',\n",
       " 'aat',\n",
       " 'cb',\n",
       " 'iction',\n",
       " 'mag',\n",
       " 'oci',\n",
       " '967',\n",
       " '852',\n",
       " '_ch',\n",
       " 'tries',\n",
       " 'zo',\n",
       " 'elli',\n",
       " '724',\n",
       " 'inf',\n",
       " '788',\n",
       " 'ár',\n",
       " '668',\n",
       " 'mi',\n",
       " 'ends',\n",
       " 'אבר',\n",
       " '845',\n",
       " 'odge',\n",
       " '&nbsp',\n",
       " 'oker',\n",
       " 'erse',\n",
       " 'أ',\n",
       " 'Que',\n",
       " 'ao',\n",
       " '844',\n",
       " 'ono',\n",
       " '081',\n",
       " 'blog',\n",
       " 'elig',\n",
       " 'chool',\n",
       " 'heim',\n",
       " 'aza',\n",
       " '649',\n",
       " 'This',\n",
       " '-off',\n",
       " 'atab',\n",
       " 'afe',\n",
       " 'ocial',\n",
       " 'gent',\n",
       " '689',\n",
       " '723',\n",
       " '814',\n",
       " 'HA',\n",
       " 'block',\n",
       " ':m',\n",
       " 'alis',\n",
       " 'cher',\n",
       " '.un',\n",
       " '764',\n",
       " 'Ge',\n",
       " 'size',\n",
       " 'uma',\n",
       " '787',\n",
       " '774',\n",
       " 'nu',\n",
       " '743',\n",
       " 'XX',\n",
       " '753',\n",
       " 'ici',\n",
       " 'oge',\n",
       " '-te',\n",
       " '726',\n",
       " 'ami',\n",
       " '883',\n",
       " '673',\n",
       " 'rot',\n",
       " '857',\n",
       " '863',\n",
       " '935',\n",
       " 'CCCC',\n",
       " '-db',\n",
       " 'ى',\n",
       " '745',\n",
       " '651',\n",
       " '767',\n",
       " 'raph',\n",
       " '865',\n",
       " '794',\n",
       " '032',\n",
       " '917',\n",
       " 'olo',\n",
       " 'SR',\n",
       " '719',\n",
       " 'ball',\n",
       " '015',\n",
       " 'lete',\n",
       " '684',\n",
       " '728',\n",
       " 'BF',\n",
       " 'Can',\n",
       " 'ateg',\n",
       " 'uh',\n",
       " 'edes',\n",
       " 'RD',\n",
       " '791',\n",
       " '643',\n",
       " '859',\n",
       " '966',\n",
       " 'raham',\n",
       " 'mall',\n",
       " '758',\n",
       " 'circle',\n",
       " 'img',\n",
       " '.ua',\n",
       " 'ा',\n",
       " '757',\n",
       " '835',\n",
       " 'HC',\n",
       " 'oda',\n",
       " '754',\n",
       " '559',\n",
       " '980',\n",
       " 'ownload',\n",
       " 'rish',\n",
       " 'AG',\n",
       " '694',\n",
       " 'awn',\n",
       " 'עבר',\n",
       " 'mm',\n",
       " '693',\n",
       " 'SM',\n",
       " 'IK',\n",
       " 'iber',\n",
       " '644',\n",
       " '.le',\n",
       " 'hit',\n",
       " 'abeth',\n",
       " '/co',\n",
       " '824',\n",
       " '672',\n",
       " '691',\n",
       " 'ports',\n",
       " '789',\n",
       " '858',\n",
       " '/fr',\n",
       " '742',\n",
       " 'label',\n",
       " 'irit',\n",
       " '061',\n",
       " '776',\n",
       " '818',\n",
       " '661',\n",
       " '736',\n",
       " 'Ph',\n",
       " 'frica',\n",
       " 'ype',\n",
       " 'etail',\n",
       " 'photo',\n",
       " '751',\n",
       " '755',\n",
       " '828',\n",
       " 'SD',\n",
       " 'new',\n",
       " 'GC',\n",
       " '847',\n",
       " 'oogle',\n",
       " '698',\n",
       " '746',\n",
       " 'formation',\n",
       " 'bin',\n",
       " 'room',\n",
       " 'table',\n",
       " '_de',\n",
       " '771',\n",
       " '.ne',\n",
       " '646',\n",
       " 'arta',\n",
       " '638',\n",
       " 'NN',\n",
       " 'lap',\n",
       " 'ого',\n",
       " '629',\n",
       " 'PM',\n",
       " '-au',\n",
       " 'itle',\n",
       " 'tails',\n",
       " 'ravel',\n",
       " '-ab',\n",
       " '>H',\n",
       " 'ough',\n",
       " '761',\n",
       " '-pr',\n",
       " 'OW',\n",
       " '675',\n",
       " '025',\n",
       " 'election',\n",
       " '587',\n",
       " '028',\n",
       " '557',\n",
       " '=k',\n",
       " '598',\n",
       " '614',\n",
       " 'vid',\n",
       " '_st',\n",
       " '091',\n",
       " 'ская',\n",
       " 'alue',\n",
       " '/display',\n",
       " 'vin',\n",
       " '721',\n",
       " 'Black',\n",
       " 'ía',\n",
       " '763',\n",
       " 'ito',\n",
       " 'nder',\n",
       " '741',\n",
       " 'SL',\n",
       " '768',\n",
       " '769',\n",
       " 'urope',\n",
       " 'eni',\n",
       " 'ру',\n",
       " 'nes',\n",
       " 'lip',\n",
       " 'elta',\n",
       " 'utt',\n",
       " '875',\n",
       " 'inners',\n",
       " '027',\n",
       " 'Art',\n",
       " 'Tr',\n",
       " 'EO',\n",
       " '653',\n",
       " '731',\n",
       " 'anti',\n",
       " '671',\n",
       " '.wordpress',\n",
       " 'arrow',\n",
       " '_title',\n",
       " 'ores',\n",
       " 'אבא',\n",
       " 'ə',\n",
       " 'pattern',\n",
       " 'itte',\n",
       " 'alo',\n",
       " '597',\n",
       " 'ache',\n",
       " '052',\n",
       " '887',\n",
       " '766',\n",
       " '816',\n",
       " 'ć',\n",
       " 'error',\n",
       " '735',\n",
       " 'Mar',\n",
       " 'itunes',\n",
       " '855',\n",
       " '685',\n",
       " 'media',\n",
       " 'aus',\n",
       " 'merican',\n",
       " 'aba',\n",
       " 'zen',\n",
       " 'arp',\n",
       " '933',\n",
       " 'ος',\n",
       " 'via',\n",
       " '918',\n",
       " '696',\n",
       " 'orf',\n",
       " '573',\n",
       " '579',\n",
       " '727',\n",
       " ':red',\n",
       " 'stone',\n",
       " 'rho',\n",
       " 'חקר',\n",
       " '775',\n",
       " 'iny',\n",
       " 'adi',\n",
       " '641',\n",
       " 'fall',\n",
       " '-Ch',\n",
       " 'base',\n",
       " '624',\n",
       " 'ills',\n",
       " '656',\n",
       " '.ge',\n",
       " 'OH',\n",
       " 'ֱ',\n",
       " '681',\n",
       " 'default',\n",
       " 'טוב',\n",
       " 'utch',\n",
       " '538',\n",
       " '628',\n",
       " '619',\n",
       " '048',\n",
       " 'tos',\n",
       " '657',\n",
       " '866',\n",
       " '749',\n",
       " 'olk',\n",
       " '815',\n",
       " 'ault',\n",
       " '-am',\n",
       " '/se',\n",
       " '626',\n",
       " 'gan',\n",
       " '552',\n",
       " 'ortal',\n",
       " 'ando',\n",
       " 'FM',\n",
       " 'TF',\n",
       " '036',\n",
       " '564',\n",
       " 'Time',\n",
       " 'PC',\n",
       " '/new',\n",
       " '595',\n",
       " '940',\n",
       " 'rix',\n",
       " '026',\n",
       " 'ordan',\n",
       " 'דנ',\n",
       " 'RC',\n",
       " 'xx',\n",
       " 'ple',\n",
       " '635',\n",
       " 'uar',\n",
       " 'Co',\n",
       " 'ouch',\n",
       " 'itzer',\n",
       " '023',\n",
       " 'ور',\n",
       " 'ام',\n",
       " 'elect',\n",
       " 'EG',\n",
       " '645',\n",
       " 'dam',\n",
       " '-group',\n",
       " 'subset',\n",
       " 'sy',\n",
       " '>R',\n",
       " 'iron',\n",
       " 'rama',\n",
       " 'png',\n",
       " '574',\n",
       " 'ices',\n",
       " 'utsch',\n",
       " '=h',\n",
       " '699',\n",
       " 'oft',\n",
       " 'oyal',\n",
       " '599',\n",
       " 'ottom',\n",
       " 'epsilon',\n",
       " '-width',\n",
       " 'ori',\n",
       " 'BI',\n",
       " 'cro',\n",
       " 'aro',\n",
       " '-ne',\n",
       " '783',\n",
       " '-St',\n",
       " 'deadline',\n",
       " 'ART',\n",
       " '715',\n",
       " '664',\n",
       " 'irus',\n",
       " 'but',\n",
       " '584',\n",
       " 'ummer',\n",
       " '016',\n",
       " 'ules',\n",
       " 'set',\n",
       " 'raine',\n",
       " '752',\n",
       " '/docs',\n",
       " '725',\n",
       " '/book',\n",
       " 'ог',\n",
       " '617',\n",
       " 'riend',\n",
       " '658',\n",
       " '716',\n",
       " '596',\n",
       " 'onia',\n",
       " 'achine',\n",
       " 'match',\n",
       " 'tz',\n",
       " '765',\n",
       " '665',\n",
       " '/home',\n",
       " 'asil',\n",
       " 'ione',\n",
       " '593',\n",
       " 'istr',\n",
       " '683',\n",
       " 'akh',\n",
       " '014',\n",
       " '.B',\n",
       " 'leases',\n",
       " 'chi',\n",
       " '>G',\n",
       " '594',\n",
       " '536',\n",
       " '.am',\n",
       " '.E',\n",
       " '.M',\n",
       " 'ING',\n",
       " \"'A\",\n",
       " '518',\n",
       " '623',\n",
       " '663',\n",
       " 'spect',\n",
       " 'fs',\n",
       " 'RIAA',\n",
       " 'asa',\n",
       " 'ifornia',\n",
       " 'ussia',\n",
       " 'isk',\n",
       " '636',\n",
       " '618',\n",
       " 'unity',\n",
       " 'aval',\n",
       " '542',\n",
       " 'ouble',\n",
       " 'reek',\n",
       " 'aylor',\n",
       " 'URL',\n",
       " '637',\n",
       " 'do',\n",
       " 'ete',\n",
       " '655',\n",
       " '627',\n",
       " 'Sch',\n",
       " 'occer',\n",
       " '/post',\n",
       " '561',\n",
       " 'gg',\n",
       " 'trans',\n",
       " 'only',\n",
       " 'layer',\n",
       " 'IAA',\n",
       " 'lood',\n",
       " 'Tube',\n",
       " '567',\n",
       " ...]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_records_df[all_records_df['predicted_layers'].str.len() != 0]['tkn'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 10915/29411 [8:44:21<16:23:27,  3.19s/it]"
     ]
    }
   ],
   "source": [
    "# filtered_cnt = filtered_cnt.progress_apply(get_token_layers, axis=1)\n",
    "\n",
    "# pickle.dump(filtered_cnt, open('data/llama3_filtered_cnt.pkl', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
